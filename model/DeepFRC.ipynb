{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20b39280",
   "metadata": {},
   "source": [
    "# <font color=blue> DeepFRC </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d2ac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Plotting Packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbn\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Interp1D\n",
    "from function.torchinterp1d import Interp1d\n",
    "# Function Helper\n",
    "from function.functions_helper_rn import r_mean_inverse, compute_signal_statistics \\\n",
    ",localization, Generate_centralized_warping_functions, plot_warped_data_analysis, SrvfClasNet\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using {device}...')\n",
    "\n",
    "# the folder where you save trained model & checkpoints\n",
    "cur_dir = os.getcwd()\n",
    "print(f\"cur_dir: {cur_dir}\")\n",
    "folder = cur_dir + '/train/medium/'\n",
    "Path(folder).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c445904c",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fa4b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "case = 'medium'\n",
    "data_dir = \"data/simulation/\" + case +\"/\"\n",
    "splitted_train_data = pd.read_csv(data_dir + \"train.csv\", header=None)\n",
    "splitted_valid_data = pd.read_csv(data_dir + \"valid.csv\", header=None)\n",
    "splitted_test_data = pd.read_csv(data_dir + \"test.csv\", header=None)\n",
    "num_train, num_valid, num_test = len(splitted_train_data), len(splitted_valid_data), len(splitted_test_data) \n",
    "num_train, num_valid, num_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaca9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_train_data[splitted_train_data.columns[-1]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f49a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_valid_data[splitted_valid_data.columns[-1]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebca9f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_test_data[splitted_test_data.columns[-1]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f48d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random shuffle\n",
    "random_seed = 3407\n",
    "np.random.seed(random_seed)\n",
    "_order_train = list(range(num_train))\n",
    "_order_valid = list(range(num_valid))\n",
    "_order_test = list(range(num_test))\n",
    "np.random.shuffle(_order_train)\n",
    "np.random.shuffle(_order_valid)\n",
    "np.random.shuffle(_order_test)\n",
    "\n",
    "splitted_train_data_np = splitted_train_data.values\n",
    "splitted_valid_data_np = splitted_valid_data.values\n",
    "splitted_test_data_np = splitted_test_data.values\n",
    "\n",
    "raw_train_data = splitted_train_data_np[_order_train, :]\n",
    "raw_valid_data = splitted_valid_data_np[_order_valid, :]\n",
    "raw_test_data = splitted_test_data_np[_order_test, :]\n",
    "\n",
    "# the last col is response variable\n",
    "data_train = raw_train_data[:, :-1]\n",
    "label_train = raw_train_data[:, [-1]]\n",
    "data_valid = raw_valid_data[:, :-1]\n",
    "label_valid = raw_valid_data[:, [-1]]\n",
    "data_test = raw_test_data[:, :-1]\n",
    "label_test = raw_test_data[:, [-1]]\n",
    "print(data_train.shape, label_train.shape)\n",
    "print(data_valid.shape, label_valid.shape)\n",
    "print(data_test.shape, label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab362570",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate((data_train, data_valid, data_test), axis=0)\n",
    "label = np.concatenate((label_train, label_valid, label_test))\n",
    "\n",
    "# Visualize\n",
    "# plot each class in one color\n",
    "num_samples = 30 \n",
    "shown_data = data[:num_samples]\n",
    "seq_len= data.shape[1] \n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x_values = np.arange(seq_len)\n",
    "for i in range(num_samples):\n",
    "    if label[i] == 1:\n",
    "        plt.plot(shown_data[i], color='royalblue')\n",
    "    else:\n",
    "        plt.plot(shown_data[i], color='orange')\n",
    "ax.set_xlabel('Sample Times')\n",
    "ax.set_ylabel('Value')\n",
    "plt.title(f'First {num_samples} Data (Original) Visualization')\n",
    "# ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfcba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize (through row direction)\n",
    "def Normalization_on_data(input_f):\n",
    "    '''\n",
    "    input_f N,C=1,L : () torch.tensor\n",
    "    Output N,C=1,L: After Z normalization\n",
    "    '''\n",
    "    length = input_f.size()[2]\n",
    "    miu = torch.mean(input_f,dim=2).repeat(1,length).unsqueeze(1)    \n",
    "    input_f = input_f - miu\n",
    "    sigma = input_f.std(dim=2,keepdim=True)    \n",
    "    input_f = input_f / sigma\n",
    "    return input_f, miu, sigma\n",
    "\n",
    "# backup raw data\n",
    "train_n, valid_n = num_train, num_valid\n",
    "raw_data_train, raw_data_valid, raw_data_test = data_train, data_valid, data_test\n",
    "raw_label_train, raw_label_valid, raw_label_test = label_train, label_valid, label_test\n",
    "# apply normalization\n",
    "data = torch.from_numpy(data).unsqueeze(1)\n",
    "norm_data, miu, sigma = Normalization_on_data(data)\n",
    "# Sanity check\n",
    "is_one = torch.isclose(norm_data.std(dim=2)[0][0], torch.tensor(1.0, dtype=norm_data.dtype), rtol=1e-05, atol=1e-08)\n",
    "if not is_one:\n",
    "    raise(\"ERROR in normalization, plz check!\")\n",
    "else:\n",
    "    print(\"Data is normalized...\")\n",
    "\n",
    "# save miu & sigma\n",
    "miu_train = miu[:train_n, :, :]\n",
    "miu_valid = miu[train_n:(train_n+valid_n), :, :]\n",
    "miu_test = miu[(train_n+valid_n):, :, :]\n",
    "sigma_train = sigma[:train_n, :, :]\n",
    "sigma_valid = sigma[train_n:(train_n+valid_n), :, :]\n",
    "sigma_test = sigma[(train_n+valid_n):, :, :]\n",
    "# print(f\"miu_train: {miu_train.shape}\\nmiu_valid: {miu_valid.shape}\\nmiu_test: {miu_test.shape}\")\n",
    "# print(f\"sigma_train: {sigma_train.shape}\\nsigma_valid: {sigma_valid.shape}\\nsigma_test: {sigma_test.shape}\")\n",
    "\n",
    "# Visualize normalized data\n",
    "norm_data_train = norm_data[:train_n, :]\n",
    "norm_data_valid = norm_data[train_n:(train_n+valid_n), :]\n",
    "norm_data_test = norm_data[(train_n+valid_n):, :]\n",
    "label_train = torch.as_tensor(raw_label_train).unsqueeze(1)\n",
    "label_valid = torch.as_tensor(raw_label_valid).unsqueeze(1)\n",
    "label_test = torch.as_tensor(raw_label_test).unsqueeze(1)\n",
    "processed_data_train = torch.cat((norm_data_train, label_train), 2)\n",
    "processed_data_valid = torch.cat((norm_data_valid, label_valid), 2)\n",
    "processed_data_test = torch.cat((norm_data_test, label_test), 2)\n",
    "processed_data_train = processed_data_train.float().to(device)\n",
    "processed_data_test = processed_data_test.float().to(device)\n",
    "processed_data_valid = processed_data_valid.float().to(device)  \n",
    "print(processed_data_train.shape, processed_data_valid.shape, processed_data_test.shape)\n",
    "\n",
    "processed_data = torch.cat((processed_data_train, processed_data_valid, processed_data_test), 0)\n",
    "processed_label = processed_data.squeeze(1)[:, -1]\n",
    "\n",
    "shown_data_normed = processed_data[:num_samples].cpu().numpy()\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x_values = np.arange(seq_len)\n",
    "for i in range(num_samples):\n",
    "    if label[i] == 1:\n",
    "        plt.plot(shown_data_normed[i, 0, :-1], color='royalblue')\n",
    "    else:\n",
    "        plt.plot(shown_data_normed[i, 0, :-1], color='orange')\n",
    "ax.set_xlabel('Sample Points')\n",
    "ax.set_ylabel('Value')\n",
    "plt.title(f'First {num_samples} Data (Normalized) Visualization')\n",
    "# ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dcece13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_fourier_coefficients(data, n=5):\n",
    "    '''\n",
    "    data: N,C,L\n",
    "    n: the number of fourier coefficients\n",
    "    '''\n",
    "    batch_size = data.shape[0]\n",
    "\n",
    "    fourier_coefficients = torch.fft.rfft(data, dim=1).to(device)\n",
    "    num_coefficients = n\n",
    "    truncated_coefficients = fourier_coefficients[:, :num_coefficients].to(device)\n",
    "    truncated_coefficients.to(device)\n",
    "\n",
    "    cosine_coefficients = truncated_coefficients.real \n",
    "    sine_coefficients = -truncated_coefficients.imag\n",
    "\n",
    "    cosine_coefficients = cosine_coefficients.to(device)\n",
    "    sine_coefficients = sine_coefficients.to(device)\n",
    "    return torch.cat([cosine_coefficients, sine_coefficients], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8a9b6e",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd249aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, d, eps=1e-6):\n",
    "        super().__init__()\n",
    "        # d is the normalization dimension\n",
    "        self.d = d\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.randn(d))\n",
    "        self.beta = nn.Parameter(torch.randn(d))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a torch.Tensor\n",
    "        # avg is the mean value of a layer\n",
    "        avg = x.mean(dim=-1, keepdim=True)\n",
    "        # std is the standard deviation of a layer (eps is added to prevent dividing by zero)\n",
    "        std = x.std(dim=-1, keepdim=True) + self.eps\n",
    "        return (x - avg) / std * self.alpha + self.beta\n",
    "\n",
    "# we deal with 2-class as example\n",
    "num_class = 2\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, in_d=1, hidden=[4,8], dropout=0.1, activation=F.relu):\n",
    "        # in_d      : input dimension, integer\n",
    "        # hidden    : hidden layer dimension, array of integers\n",
    "        # dropout   : dropout probability, a float between 0.0 and 1.0\n",
    "        # activation: activation function at each layer\n",
    "        super().__init__()\n",
    "        self.sigma = activation\n",
    "        dim = [in_d] + hidden + [num_class]\n",
    "        self.layers = nn.ModuleList([nn.Linear(dim[i-1], dim[i]) for i in range(1, len(dim))])\n",
    "        self.ln = nn.ModuleList([LayerNorm(k) for k in hidden])\n",
    "        self.dp = nn.ModuleList([nn.Dropout(dropout) for _ in range(len(hidden))])\n",
    "\n",
    "    def forward(self, t):\n",
    "        for i in range(len(self.layers)-1):\n",
    "            t = self.layers[i](t)\n",
    "            # skipping connection\n",
    "            t = t + self.ln[i](t)\n",
    "            t = self.sigma(t)\n",
    "            # apply dropout\n",
    "            t = self.dp[i](t)\n",
    "        # just return the logits since we will use CrossEntropyLoss which contains nn.LogSoftmax() and nn.NLLLoss()\n",
    "        return self.layers[-1](t)\n",
    "\n",
    "def hidden_to_fc_calculation(input_f,localization=localization):\n",
    "    '''\n",
    "    input_f : The input data\n",
    "    localization : Layers to generate hidden features\n",
    "    '''\n",
    "    vv = localization(input_f.cpu().float()).detach()\n",
    "    vv = vv.view(input_f.size(0), -1)\n",
    "    return vv.size(-1), input_f.size(-1)\n",
    "\n",
    "hidden_to_fc, fun_len = hidden_to_fc_calculation(processed_data_train[:, :, :-1])\n",
    "\n",
    "# Set srvf params\n",
    "Srvf_params = {\n",
    "    \"hidden_to_fc\": hidden_to_fc,\n",
    "    \"fun_len\": fun_len,\n",
    "    \"Interp1d\": Interp1d,\n",
    "    \"device\": device\n",
    "}\n",
    "\n",
    "# Set ff params\n",
    "FF_params = {\n",
    "    \"in_d\": 100,\n",
    "    \"hidden\": [16, 8],\n",
    "    \"dropout\": 0.1\n",
    "}\n",
    "\n",
    "# RegisNet\n",
    "class regisNet(nn.Module):\n",
    "    def __init__(self, Srvf_params, FF_params):\n",
    "        super().__init__()\n",
    "        self.srvfNet = SrvfClasNet(**Srvf_params)\n",
    "        self.classifier = FeedForward(**FF_params)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # Warped Q, gamma\n",
    "        Q, f_r, gamma = self.srvfNet(input)\n",
    "        seris = to_fourier_coefficients(f_r,50)\n",
    "        seris = seris\n",
    "        # the forward pass return \"un-centered gamma\"\n",
    "        # the warped sequence (NOT in Srvf space!) are fed into FCN module\n",
    "        #print(f\"shape of seris: {seris.shape}\")\n",
    "        y = self.classifier(seris)\n",
    "        return y, Q, gamma, f_r\n",
    "    \n",
    "model = regisNet(Srvf_params, FF_params).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef33d288",
   "metadata": {},
   "source": [
    "# Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfa7e6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(folder, k, Srvf_params, FF_params, model, optimizer):\n",
    "    checkpoint = {'Srvf_params': Srvf_params,\n",
    "                  'FF_params': FF_params,\n",
    "                  'state_dict': model.state_dict(),\n",
    "                  'optimizer': optimizer.state_dict()}\n",
    "    torch.save(checkpoint, folder + str(k) + '_' + 'checkpoint.pth')\n",
    "\n",
    "def load_model(file_path, device):\n",
    "    checkpoint = torch.load(file_path)\n",
    "    model = regisNet(Srvf_params=checkpoint['Srvf_params'],\n",
    "                            FF_params=checkpoint['FF_params'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    _ = model.to(device)\n",
    "    return model\n",
    "\n",
    "###########################################################################\n",
    "alpha_1 = 0.1 # 600\n",
    "alpha_2 = 1 # 0, 1, 10, 20, 25\n",
    "num_epochs = 300\n",
    "batch_size = processed_data_train.size(0) # batch size need to be small or equal to sample size \n",
    "learning_rate_srvf = 0.001 # 0.0015, 0.0012, 0.001, 3e-3\n",
    "learning_rate_fcn = 0.001 #0.005 0.0015\n",
    "params_dict = [{'params': model.srvfNet.parameters()  , 'lr': learning_rate_srvf},              \n",
    "             {'params': model.classifier.parameters(), 'lr': learning_rate_fcn}]\n",
    "criterion        = nn.CrossEntropyLoss()\n",
    "optimizer        = torch.optim.AdamW(params_dict)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.9)\n",
    "save_model_every = 1\n",
    "seq_len          = fun_len\n",
    "mse_criterion    = nn.MSELoss(reduction='mean')\n",
    "###########################################################################\n",
    "alpha = alpha_1\n",
    "lamda = alpha_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41790075",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping_Fire_Down:\n",
    "    def __init__(self, total_patience=20, loss_min_delta=0.001, acc_min_delta=0.01, regis_patience=15, regis_min_delta=0.00045, classify_min_delta=0.1):\n",
    "        ###############################\n",
    "        # control stop\n",
    "        self.patience = total_patience\n",
    "        self.counter = 0\n",
    "        self.stop = False\n",
    "        # moniter loss change\n",
    "        self.loss_min_delta = loss_min_delta\n",
    "        self.best_sub_loss = None\n",
    "        self.best_loss = None # this track the total loss\n",
    "        # moniter acc change\n",
    "        self.acc_min_delta = acc_min_delta\n",
    "        self.best_acc = None # this track the acc\n",
    "        ###############################\n",
    "        # control freeze\n",
    "        self.regis_patience = regis_patience\n",
    "        self.regis_min_delta = regis_min_delta\n",
    "        self.regis_counter = 0\n",
    "        self.best_regis_loss = None # this track the regis loss (fire down when regis module is improved to bottleneck)\n",
    "        self.best_ce_loss = None\n",
    "        self.freeze = False\n",
    "        self.classify_min_delta = classify_min_delta\n",
    "        ###############################\n",
    "\n",
    "    def __call__(self, subLoss_val, val_loss, regis_loss, valid_acc, ce_loss):\n",
    "        #####################################################################################\n",
    "        if self.best_sub_loss is None:\n",
    "            self.best_sub_loss = subLoss_val\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        if self.best_acc is None:\n",
    "            self.best_acc = valid_acc\n",
    "        if self.best_ce_loss is None:\n",
    "            self.best_ce_loss = ce_loss\n",
    "\n",
    "        valid_sub_loss_delta = self.best_sub_loss - subLoss_val\n",
    "        valid_loss_delta = self.best_loss - val_loss\n",
    "        valid_acc_delta = valid_acc - self.best_acc\n",
    "        valid_ce_loss_delta = self.best_ce_loss - ce_loss\n",
    "        ###################################### Control Freeze #######################################\n",
    "        # update ce_loss\n",
    "        if valid_ce_loss_delta > 0:\n",
    "            self.best_ce_loss = ce_loss\n",
    "        # Consider classification (if regis loss gets lower but CE gets higher / (acc drops), we should also stop updates on whole network)\n",
    "        # instead, we need to focus on classification now\n",
    "        if self.freeze==False:\n",
    "            if self.best_regis_loss is None:\n",
    "                self.best_regis_loss = regis_loss\n",
    "            elif self.best_regis_loss > regis_loss and self.best_regis_loss - regis_loss <= self.regis_min_delta: # regis_loss gets better\n",
    "                self.best_regis_loss = regis_loss\n",
    "                if valid_ce_loss_delta < -self.loss_min_delta * 2:\n",
    "                    self.regis_counter += 1\n",
    "            elif self.best_regis_loss - regis_loss > self.regis_min_delta: # regis_loss gets way better\n",
    "                self.best_regis_loss = regis_loss\n",
    "                self.regis_counter = 0 # reset freeze_counter\n",
    "                if valid_ce_loss_delta < -self.loss_min_delta * 2:\n",
    "                    self.regis_counter += 1\n",
    "            else: # if regis_loss gets worse\n",
    "                self.regis_counter += 1\n",
    "                if self.regis_counter >= self.regis_patience:\n",
    "                    self.freeze = True\n",
    "                    # counter \n",
    "                    self.counter = 0\n",
    "                    self.loss_min_delta = self.classify_min_delta\n",
    "                    \n",
    "        #######################################################################################\n",
    "        #######################################################################################\n",
    "        if valid_acc_delta > 0 and valid_loss_delta > 0:\n",
    "            self.best_acc = valid_acc\n",
    "            self.best_loss = val_loss\n",
    "            if valid_acc_delta > self.acc_min_delta and valid_loss_delta > self.loss_min_delta:\n",
    "                self.counter = 0 \n",
    "\n",
    "        elif valid_acc_delta > 0 and valid_loss_delta <= 0:\n",
    "            self.best_acc = valid_acc # update acc\n",
    "            if valid_acc_delta > self.acc_min_delta * 2:\n",
    "                self.counter -= 1  # back\n",
    "\n",
    "        elif valid_acc_delta <= 0 and valid_loss_delta > 0:\n",
    "            self.best_loss = val_loss # update loss\n",
    "            if valid_loss_delta > self.loss_min_delta * 2: \n",
    "                self.counter -= 1  # back\n",
    "            if valid_ce_loss_delta < -self.loss_min_delta * 2: \n",
    "                self.counter += 1 \n",
    "\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.stop = True  # when total valid loss can't improve, stop training\n",
    "        ##############################################################################\n",
    "        # track sub_loss\n",
    "        if valid_sub_loss_delta > 0:\n",
    "            self.best_sub_loss = subLoss_val\n",
    "        #################### if in finetuning classifier (frozen) ####################\n",
    "        if self.freeze:\n",
    "            if valid_loss_delta < 0:\n",
    "                self.counter += 1\n",
    "\n",
    "early_stopping_firedown = EarlyStopping_Fire_Down(total_patience=120, loss_min_delta=0.5, acc_min_delta=0.01, regis_patience=80, regis_min_delta=0.5, classify_min_delta=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e26df7",
   "metadata": {},
   "source": [
    "(Optional) visualize Q before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8faf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(f):\n",
    "        \"\"\"\n",
    "        Central difference method with equal space in dx(i.e.1)\n",
    "        Input : f in tensor (batch size x fun len)\n",
    "        Output : gradient along axis fun len(dim=1)\n",
    "\n",
    "        \"\"\"\n",
    "        row_num = f.ndim  # y.size()[0]\n",
    "        dx = torch.arange(f.size()[1])  # only 1d\n",
    "        output = torch.empty_like(f)\n",
    "        # i=0\n",
    "        output[:, 0] = torch.div(\n",
    "            f[:, 1] - f[:, 0], dx[1] - dx[0]\n",
    "        )  # y[:,1] - y[:,0]/ dx[1]-dx[0]\n",
    "        # i = end\n",
    "        output[:, -1] = torch.div(\n",
    "            f[:, -1] - f[:, -2], dx[-1] - dx[-2]\n",
    "        )  # y[:,-1] - y[:,-2]/  dx[-1]-dx[-2]\n",
    "        # i = 1:end-1\n",
    "        for i in range(1, len(dx) - 1):\n",
    "            output[:, i] = torch.div(f[:, i - 1] - f[:, i + 1], dx[i - 1] - dx[i + 1])\n",
    "        return output\n",
    "\n",
    "def srvf(v, gradient):\n",
    "        \"\"\"\n",
    "        v (batch size x fun len )\n",
    "        q = sign(f')* sqrt(|f'|)\n",
    "        No backward\n",
    "        \"\"\"\n",
    "        f = v.detach()\n",
    "        q = torch.empty_like(f)\n",
    "        batch, length = f.size()\n",
    "        length_adjust = length - 1\n",
    "        grad_f = gradient(f * length_adjust)\n",
    "        for i in range(batch):\n",
    "            q[i, :] = torch.sign(grad_f[i, :]) * torch.sqrt(grad_f[i, :].abs())\n",
    "        return q\n",
    "\n",
    "batch_data = processed_data_train\n",
    "# r is the gamma function\n",
    "# Q is the warped data (in srvf space)\n",
    "bat_data = batch_data[:, :, :-1]\n",
    "bat_data_shape = bat_data.shape\n",
    "# bat_label = batch_data[:, :, [-1]]\n",
    "bat_label = batch_data[:, :, -1]\n",
    "bat_label_shape = bat_label.shape\n",
    "# divide batch_data into two parts now: data_A and data_B based on their label\n",
    "data_A = bat_data[bat_label == 0]\n",
    "data_B = bat_data[bat_label == 1]\n",
    "data_A_shape = data_A.shape\n",
    "data_B_shape = data_B.shape\n",
    "data_A = data_A.unsqueeze(1)\n",
    "data_B = data_B.unsqueeze(1)\n",
    "\n",
    "length_adjust = seq_len - 1\n",
    "q_A = srvf(data_A.squeeze(1), gradient)\n",
    "q_B = srvf(data_B.squeeze(1), gradient)\n",
    "\n",
    "# plot all the curve in q_A in one color\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(q_A.shape[0]):\n",
    "    plt.plot(q_A.cpu()[i, :], color='orange')\n",
    "\n",
    "# plot all the curve in q_B in one color\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(q_B.shape[0]):\n",
    "    plt.plot(q_B.cpu()[i, :], color='royalblue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0e0972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot q_mean_A and q_mean_B in one plot but with different color\n",
    "q_mean_A = q_A.mean(0, keepdim=True)\n",
    "q_mean_B = q_B.mean(0, keepdim=True)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(q_mean_A.cpu().squeeze(0), color='orange')\n",
    "plt.plot(q_mean_B.cpu().squeeze(0), color='royalblue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4d1bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_trainval(\n",
    "    model,\n",
    "    batch_size,\n",
    "    num_epochs,\n",
    "    data_train,\n",
    "    data_val,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    device,\n",
    "    schedular,\n",
    "    lamda,\n",
    "    subLoss_val=[],\n",
    "    loss_val=[],\n",
    "    loss_train=[],\n",
    "    left_loss_train=[],\n",
    "    right_loss_train=[],\n",
    "    penal_1_train=[],\n",
    "    acc_val=[],\n",
    "    regis_loss_valid=[],\n",
    "    regisA_loss_valid=[],\n",
    "    regisB_loss_valid=[],\n",
    "    regisA_loss_train=[],\n",
    "    regisB_loss_train=[],\n",
    "    classify_loss_valid=[],\n",
    "    Q_A_train_history=[],\n",
    "    Q_B_train_history=[],\n",
    "    Q_A_mean_train_history=[],\n",
    "    Q_B_mean_train_history=[],\n",
    "    Q_A_valid_history=[],\n",
    "    Q_B_valid_history=[],\n",
    "    Q_A_mean_valid_history=[],\n",
    "    Q_B_mean_valid_history=[],\n",
    "    visualize_Q_for_n_times=10, \n",
    "    time=time,\n",
    "):\n",
    "    \"\"\"\n",
    "    model : The model uses\n",
    "    batch_size : For both training and validation\n",
    "    criterion : CrossEntropyLoss\n",
    "    device : cpu or cuda\n",
    "    schedular : learing rate decay\n",
    "    loss_val : The record of validation loss\n",
    "    loss_train : The record of training loss\n",
    "    \"\"\"\n",
    "    opt = optimizer\n",
    "    best_model = None\n",
    "    best_val = 10000\n",
    "    best_train = -1\n",
    "    best_epoch = 1\n",
    "    since = time.time()\n",
    "    magic_token = True\n",
    "    control_penalty_mse = alpha # 20\n",
    "    mac = 1\n",
    "\n",
    "    # In case batch size > test data size\n",
    "    if batch_size > data_val.size()[0]:\n",
    "        val_batch_size = data_val.size()[0]\n",
    "    else:\n",
    "        val_batch_size = batch_size\n",
    "\n",
    "    # model.to(device)\n",
    "    num_iteration_epoch_train = int(data_train.size()[0] / batch_size)\n",
    "    num_iteration_epoch_val = int(data_val.size()[0] / val_batch_size)\n",
    "\n",
    "    # Save best model\n",
    "    # best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    visual_Q_every = num_epochs // visualize_Q_for_n_times\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        visualize_Q = False\n",
    "        if epoch and epoch % save_model_every == 0:\n",
    "            save_model(\n",
    "                folder=folder,\n",
    "                k=epoch,\n",
    "                Srvf_params=Srvf_params,\n",
    "                FF_params=FF_params,\n",
    "                model=model,\n",
    "                optimizer=opt,\n",
    "            )\n",
    "        if epoch and epoch % visual_Q_every == 0:\n",
    "            visualize_Q = True\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"val\", \"train\"]:\n",
    "            # val first, train later\n",
    "            # \"Validation\"\n",
    "            if phase == \"val\":\n",
    "                with torch.no_grad():\n",
    "                    model.eval()  # Set model to validation mode\n",
    "                    loss_epoch_val = 0.0\n",
    "                    regis_loss_epoch_val = 0.0\n",
    "                    regisA_loss_epoch_val = 0.0\n",
    "                    regisB_loss_epoch_val = 0.0\n",
    "                    classify_loss_epoch_val = 0.0\n",
    "                    subLoss_epoch_val = 0.0\n",
    "                    acc_epoch_val = 0.0\n",
    "                    valdata_shuffle = data_val\n",
    "                    # we don't need to shuffle the valid set\n",
    "                    # [torch.randperm(data_val.size()[0]),:,:]\n",
    "                    # size: (val_batch_size, 1, 99+1)\n",
    "\n",
    "                    for iteration in range(num_iteration_epoch_val):\n",
    "                        batch_data = valdata_shuffle[\n",
    "                            val_batch_size\n",
    "                            * (iteration) : val_batch_size\n",
    "                            * (iteration + 1),\n",
    "                            :,\n",
    "                            :,\n",
    "                        ].to(device)\n",
    "\n",
    "                        batch_data_shape = batch_data.shape\n",
    "                        # print(f\"batch_data_shape: {batch_data_shape}\")\n",
    "                        # (64, 1, 100)\n",
    "\n",
    "                        # r is the gamma function\n",
    "                        # Q is the warped data (in srvf space)\n",
    "\n",
    "                        # batch_data: (batch_size, fun_len+1)\n",
    "                        # We need to divide them into data and label\n",
    "                        bat_data = batch_data[:, :, :-1]\n",
    "\n",
    "                        # bat_label = batch_data[:, :, [-1]]\n",
    "                        bat_label = batch_data[:, :, -1]\n",
    "                        bat_label_shape = bat_label.shape\n",
    "                        # size: (val_batch_size, 1)\n",
    "\n",
    "                        # divide batch_data into two parts now: data_A and data_B based on their label\n",
    "                        data_A = bat_data[bat_label == 0]\n",
    "                        data_B = bat_data[bat_label == 1]\n",
    "                        # remember to keep the dim\n",
    "                        data_A = data_A.unsqueeze(1)\n",
    "                        data_B = data_B.unsqueeze(1)\n",
    "\n",
    "                        # print(f\"data_A_shape: {data_A_shape}\")\n",
    "                        # print(f\"data_B_shape: {data_B_shape}\")\n",
    "                        # print(f\"bat_data_shape: {bat_data.shape}\")\n",
    "                        #################################################################################\n",
    "                        # for for all the data we divide them by label\n",
    "                        #### the first part: label = 0\n",
    "                        y_bar_A, Q_A, r_A, f_r_A = model(data_A)\n",
    "                        #### the second part: label = 1\n",
    "                        y_bar_B, Q_B, r_b, f_r_B = model(data_B)\n",
    "\n",
    "                        #################################################################################\n",
    "                        # Q_mean_A_single = Q_A.mean(0).detach()\n",
    "                        # Q_mean_B_single = Q_B.mean(0).detach()\n",
    "                        Q_mean_A_single = Q_A.mean(0)\n",
    "                        Q_mean_B_single = Q_B.mean(0)\n",
    "                        if visualize_Q:\n",
    "                            Q_A_mean_valid_history.append(Q_mean_A_single)\n",
    "                            Q_B_mean_valid_history.append(Q_mean_B_single)\n",
    "                            Q_A_valid_history.append(Q_A)\n",
    "                            Q_B_valid_history.append(Q_B)\n",
    "                            \n",
    "                        ############# Compute Loss #############\n",
    "                        # Q_mean's size is (val_batch_size, fun_len=99)\n",
    "\n",
    "                        # now when we compute loss, there are two parts\n",
    "                        # get the Q_mean of data_A and data_B\n",
    "                        Q_mean_A = Q_mean_A_single.repeat(data_A.shape[0], 1)\n",
    "                        Q_mean_B = Q_mean_B_single.repeat(data_B.shape[0], 1)\n",
    "                        left_loss_A = mse_criterion(Q_A, Q_mean_A)\n",
    "                        left_loss_B = mse_criterion(Q_B, Q_mean_B)\n",
    "                        left_loss = left_loss_A + left_loss_B\n",
    "\n",
    "                        # here is loss for one single batch\n",
    "                        y_A = bat_label[bat_label == 0]\n",
    "                        y_B = bat_label[bat_label == 1]\n",
    "\n",
    "                        \"\"\"\n",
    "                        print(f\"y_A_shape: {y_A.shape}\")\n",
    "                        print(f\"y_B_shape: {y_B.shape}\")\n",
    "                        print(f\"y_bar_A_shape: {y_bar_A.shape}\")\n",
    "                        print(f\"y_bar_B_shape: {y_bar_B.shape}\")\n",
    "                        \"\"\"\n",
    "                        # check dtype\n",
    "                        # target should be long while prediction should be float (for crossentropy)\n",
    "                        y_A = y_A.long()\n",
    "                        y_B = y_B.long()\n",
    "                        \"\"\"\n",
    "                        print(f\"y_A dtype: {y_A.dtype}\")\n",
    "                        print(f\"y_B dtype: {y_B.dtype}\")\n",
    "                        print(f\"y_bar_A dtype: {y_bar_A.dtype}\")\n",
    "                        print(f\"y_bar_B dtype: {y_bar_B.dtype}\")\n",
    "                        \"\"\"\n",
    "                        # crossentropy\n",
    "                        right_loss_A = criterion(y_bar_A, y_A)\n",
    "                        right_loss_B = criterion(y_bar_B, y_B)\n",
    "                        right_loss = right_loss_A + right_loss_B\n",
    "                        # loss_epoch_val += left_loss.item() + lamda * right_loss.item()\n",
    "                        ####### [regis loss]    [classification loss]    \n",
    "                        loss = mac * left_loss + lamda * right_loss\n",
    "                        ########################################\n",
    "                        # We add some penalty term to the loss\n",
    "                        # we want Q_mean_A and Q_mean_B to be different (i.e. the worst case is they are identical)\n",
    "                        # so we need a penalty term to represent the difference between Q_mean_A and Q_mean_B\n",
    "                        # we use MSE here\n",
    "                        diff_dist = mse_criterion(Q_mean_A_single, Q_mean_B_single)\n",
    "                        # when diff_dist is smaller, the penalty should be larger\n",
    "                        # so we use 1 / diff_dist\n",
    "                        penalty = 1 / diff_dist\n",
    "                        # add penalty to the loss\n",
    "                        ##############################################\n",
    "                        # control_p = 15\n",
    "                        ##############################################\n",
    "                        penalty_1 = penalty * control_penalty_mse\n",
    "                        \n",
    "                        \"\"\"\n",
    "                        # Compute cross-correlation\n",
    "                        # cross_corr = correlate(Q_mean_B, Q_mean_A, mode='full')\n",
    "                        cross_corr = correlate(Q_mean_B_single.cpu(), Q_mean_A_single.cpu(), mode='full')\n",
    "                        similarity = cross_corr.max()\n",
    "                        # control_s = 0.001\n",
    "                        penalty_2 = similarity * control_penalty_cor\n",
    "                        \"\"\"\n",
    "                        # penalty_total = penalty_1 + penalty_2\n",
    "                        penalty_total = penalty_1\n",
    "                        loss += penalty_total\n",
    "                        ########################################\n",
    "                        sub_loss = left_loss + penalty_total # omit classification (might be useful before freezing)\n",
    "                        subLoss_epoch_val += sub_loss.item()\n",
    "                        ########################################\n",
    "\n",
    "                        loss_epoch_val += loss.item()\n",
    "                        regis_loss_epoch_val += left_loss.item()\n",
    "                        # print(f\"regis_loss_epoch_val: {regis_loss_epoch_val}\")\n",
    "                        regisA_loss_epoch_val += left_loss_A.item()\n",
    "                        regisB_loss_epoch_val += left_loss_B.item()\n",
    "                        classify_loss_epoch_val += right_loss.item()\n",
    "\n",
    "                        # calculate accuracy\n",
    "                        y_bar_A = y_bar_A.squeeze(1)\n",
    "                        y_bar_B = y_bar_B.squeeze(1)\n",
    "                        # the index of the largest number in the output logits will be our prediction\n",
    "                        _, pred_A = torch.max(y_bar_A, 1)\n",
    "                        _, pred_B = torch.max(y_bar_B, 1)\n",
    "                        \"\"\"\n",
    "                        print(f\"pred_A: {pred_A}\")\n",
    "                        print(f\"pred_B: {pred_B}\")\n",
    "                        print(f\"y_bar_A: {y_bar_A}\")\n",
    "                        print(f\"y_bar_B: {y_bar_B}\")\n",
    "                        print(f\"y_actual_A: {y_A}\")\n",
    "                        print(f\"y_actual_B: {y_B}\")\n",
    "                        \"\"\"\n",
    "\n",
    "                        corrected_A = (pred_A == 0).sum().item()\n",
    "                        corrected_B = (pred_B == 1).sum().item()\n",
    "\n",
    "                        total_num = bat_data.size(0)\n",
    "                        # get accuracy\n",
    "                        acc = (corrected_A + corrected_B) / total_num\n",
    "                        acc_epoch_val += acc\n",
    "\n",
    "                    # reocrd epoch per loss\n",
    "                    subLoss_val.append(subLoss_epoch_val / (iteration + 1))\n",
    "                    loss_val.append(loss_epoch_val / (iteration + 1))\n",
    "                    regis_loss_valid.append(regis_loss_epoch_val / (iteration + 1))\n",
    "                    regisA_loss_valid.append(regisA_loss_epoch_val / (iteration + 1))\n",
    "                    regisB_loss_valid.append(regisB_loss_epoch_val / (iteration + 1))\n",
    "                    classify_loss_valid.append(classify_loss_epoch_val / (iteration + 1))\n",
    "                    acc_val.append(acc_epoch_val / (iteration + 1))\n",
    "            # \"Train\"\n",
    "            else:\n",
    "                # add learning rate decay\n",
    "                # if schedular != None:\n",
    "                model.train()  # Set model to train mode\n",
    "                loss_epoch_train = 0.0\n",
    "                ##############################\n",
    "                # Trace each loss's variation trend\n",
    "                left_loss_epoch_train = 0.0\n",
    "                regisA_loss_epoch_train = 0.0\n",
    "                regisB_loss_epoch_train = 0.0\n",
    "                right_loss_epoch_train = 0.0\n",
    "                ##############################\n",
    "                penal_1_epoch_train = 0.0\n",
    "                penal_2_epoch_train = 0.0\n",
    "                ##############################\n",
    "                # shuffle the training set\n",
    "                # Here I mute shuffle because I do shuffling on train set before I call this train function\n",
    "                trainingdata_shuffle = data_train\n",
    "                # trainingdata_shuffle = data_train[torch.randperm(data_train.size()[0]), :, :]\n",
    "\n",
    "                for iteration in range(num_iteration_epoch_train):\n",
    "                    batch_data = trainingdata_shuffle[\n",
    "                        batch_size * (iteration) : batch_size * (iteration + 1), :, :\n",
    "                    ].to(device)\n",
    "                    ###########################\n",
    "\n",
    "                    bat_data = batch_data[:, :, :-1]\n",
    "                    bat_data_shape = bat_data.shape\n",
    "                    # (64, 1, 99)\n",
    "\n",
    "                    # bat_label = batch_data[:, :, [-1]]\n",
    "                    bat_label = batch_data[:, :, -1]\n",
    "                    bat_label_shape = bat_label.shape\n",
    "                    # size: (val_batch_size, 1)\n",
    "                    # (64, 1)\n",
    "\n",
    "                    # divide batch_data into two parts now: data_A and data_B based on their label\n",
    "                    data_A = bat_data[bat_label == 0]\n",
    "                    data_B = bat_data[bat_label == 1]\n",
    "                    # remember to keep the dim\n",
    "                    data_A = data_A.unsqueeze(1)\n",
    "                    data_B = data_B.unsqueeze(1)\n",
    "\n",
    "                    ###########################\n",
    "\n",
    "                    opt.zero_grad()\n",
    "\n",
    "                    ##### the first part: label = 0\n",
    "                    #print(data_A.shape)\n",
    "                    y_bar_A, Q_A, r_A, f_r_A = model(data_A)    # Q_A & Q_B is the srvf form of warped f, so they're related to parameters' updating\n",
    "                    #### the second part: label = 1\n",
    "                    # print(data_B.shape)\n",
    "                    y_bar_B, Q_B, r_b, f_r_B = model(data_B)\n",
    "\n",
    "                    # now when we compute loss, there are two parts\n",
    "                    # get the Q_mean of data_A and data_B (dynamic target)\n",
    "                    ############################################################################\n",
    "                    ############################################################################\n",
    "                    ############################################################################\n",
    "                    # therefore, Q_A_mean and Q_B_mean also change along epoch changes\n",
    "                    # and the penalty term ensures the two targets are different\n",
    "                    # Q_mean_A_single = Q_A.mean(0).detach()\n",
    "                    Q_mean_A_single = Q_A.mean(0)\n",
    "                    Q_mean_A = Q_mean_A_single.repeat(data_A.shape[0], 1)\n",
    "\n",
    "                    # Q_mean_B_single = Q_B.mean(0).detach()\n",
    "                    Q_mean_B_single = Q_B.mean(0)\n",
    "                    Q_mean_B = Q_mean_B_single.repeat(data_B.shape[0], 1)\n",
    "\n",
    "                    if visualize_Q:\n",
    "                        Q_A_mean_train_history.append(Q_mean_A_single)\n",
    "                        Q_B_mean_train_history.append(Q_mean_B_single)\n",
    "                        Q_A_train_history.append(Q_A)\n",
    "                        Q_B_train_history.append(Q_B)\n",
    "\n",
    "                    ############################################################################\n",
    "                    ############################################################################\n",
    "                    ############################################################################\n",
    "                    \"\"\"\n",
    "                    if epoch == 0:\n",
    "                        Q_A_start = Q_A_start.repeat(data_A.shape[0], 1)\n",
    "                        Q_B_start = Q_B_start.repeat(data_B.shape[0], 1)\n",
    "                        # print(f\"Q_A_start's shape: {Q_A_start.shape}\")\n",
    "                        # print(f\"Q_B_start's shape: {Q_B_start.shape}\")\n",
    "                    if epoch<10:  #80\n",
    "                        left_loss_A = mse_criterion(Q_A, Q_A_start)\n",
    "                        left_loss_B = mse_criterion(Q_B, Q_B_start)\n",
    "                    else:\n",
    "                        left_loss_A = mse_criterion(Q_A, Q_mean_A)\n",
    "                        left_loss_B = mse_criterion(Q_B, Q_mean_B)\n",
    "                    \"\"\"\n",
    "                    left_loss_A = mse_criterion(Q_A, Q_mean_A)\n",
    "                    left_loss_B = mse_criterion(Q_B, Q_mean_B)\n",
    "                    ############################################################################\n",
    "                    regisA_loss_epoch_train += left_loss_A.item()\n",
    "                    regisB_loss_epoch_train += left_loss_B.item()\n",
    "                    \n",
    "                    left_loss = left_loss_A + left_loss_B\n",
    "\n",
    "                    # here is loss for one single batch\n",
    "                    y_A = bat_label[bat_label == 0]\n",
    "                    y_B = bat_label[bat_label == 1]\n",
    "                    # check dtype\n",
    "                    # target should be long while prediction should be float (for crossentropy)\n",
    "                    y_A = y_A.long()\n",
    "                    y_B = y_B.long()\n",
    "\n",
    "                    # crossentropy\n",
    "                    right_loss_A = criterion(y_bar_A, y_A)\n",
    "                    right_loss_B = criterion(y_bar_B, y_B)\n",
    "                    right_loss = right_loss_A + right_loss_B\n",
    "\n",
    "                    loss = mac * left_loss + lamda * right_loss\n",
    "\n",
    "                    ########################################################################################################################\n",
    "                    ########################################################################################################################\n",
    "                    ########################################################################################################################\n",
    "                    # Now we add some penalty term to the loss\n",
    "                    # we want Q_mean_A and Q_mean_B to be different (i.e. the worst case is they are identical)\n",
    "                    # so we need a penalty term to represent the difference between Q_mean_A and Q_mean_B\n",
    "                    # we use MSE here\n",
    "                    diff_dist = mse_criterion(Q_mean_A_single, Q_mean_B_single)\n",
    "                    # when diff_dist is smaller, the penalty should be larger\n",
    "                    # so we use 1 / diff_dist\n",
    "                    penalty = 0.1 / diff_dist\n",
    "                    # add penalty to the loss\n",
    "                    ##############################################\n",
    "                    # control_p = 15\n",
    "                    ##############################################\n",
    "                    penalty_1 = penalty * control_penalty_mse\n",
    "                    \n",
    "                    \"\"\"\n",
    "                    # Compute cross-correlation\n",
    "                    # cross_corr = correlate(Q_mean_B, Q_mean_A, mode='full')\n",
    "                    cross_corr = correlate(Q_mean_B_single.cpu(), Q_mean_A_single.cpu(), mode='full')\n",
    "                    similarity = cross_corr.max()\n",
    "                    # control_s = 0.001\n",
    "                    # control_s = 0.001\n",
    "                    penalty_2 = similarity * control_penalty_cor\n",
    "                    \"\"\"\n",
    "                    # penalty_total = penalty_1 + penalty_2\n",
    "                    penalty_total = penalty_1\n",
    "                    loss += penalty_total\n",
    "                    #######################\n",
    "                    loss_epoch_train += loss.item()\n",
    "                    #######################\n",
    "                    left_loss_epoch_train += left_loss.item()\n",
    "                    right_loss_epoch_train += right_loss.item()\n",
    "                    #######################\n",
    "                    penal_1_epoch_train += penalty_1.item()\n",
    "                    #######################\n",
    "\n",
    "                    # loss_epoch_train += loss.item()\n",
    "                    loss.backward()\n",
    "                    opt.step()\n",
    "\n",
    "                    # leaning rate decay\n",
    "                    if schedular != None:\n",
    "                        schedular.step()\n",
    "\n",
    "                # reocrd epoch per loss\n",
    "                n_itr = iteration + 1\n",
    "                # print(f'n_itr={n_itr}')\n",
    "                loss_train.append(loss_epoch_train / n_itr)\n",
    "                left_loss_train.append(left_loss_epoch_train / n_itr)\n",
    "                regisA_loss_train.append(regisA_loss_epoch_train / (iteration + 1))\n",
    "                regisB_loss_train.append(regisB_loss_epoch_train / (iteration + 1))\n",
    "                right_loss_train.append(right_loss_epoch_train / n_itr)\n",
    "\n",
    "                penal_1_train.append(penal_1_epoch_train / n_itr)\n",
    "\n",
    "        ########################################\n",
    "        # Early Stopping (monitor both total valid loss and regis loss to control train schedule)\n",
    "        early_stopping_firedown(subLoss_val[-1], loss_val[-1], regis_loss_valid[-1], acc_val[-1], classify_loss_valid[-1])\n",
    "\n",
    "        if early_stopping_firedown.stop:\n",
    "            print(\" === Early stopping triggered === \")\n",
    "            break\n",
    "        ########################################\n",
    "        ########################################\n",
    "        # FREEZE\n",
    "        if early_stopping_firedown.freeze and magic_token:\n",
    "            print(\"Freezing updates for regis module ~~--z_ _ _ _\")\n",
    "            magic_token = False\n",
    "            # one time magic\n",
    "            ###############################################################################################################\n",
    "            # first of all, save the current model \"right_before_freeze.pth\" for check effectiveness\n",
    "            save_model(folder, \"right_before_freeze\", Srvf_params, FF_params, model, opt)\n",
    "            ###############################################################################################################\n",
    "            # retrive the current best model\n",
    "            # load\n",
    "            \n",
    "            # ck_based_on_xxx = folder + \"best_checkpoint.pth\"\n",
    "            ck_based_on_xxx = folder + \"best_subLoss_checkpoint.pth\"\n",
    "            \n",
    "            # ck_based_on_xxx = folder + \"best_checkpoint.pth\"\n",
    "            # since it's still half of training, we focus on loss here\n",
    "            model = load_model(ck_based_on_xxx, device)\n",
    "            # train this model from now on\n",
    "\n",
    "            for param in model.srvfNet.parameters():\n",
    "                param.requires_grad = False\n",
    "                # stop updating regis module\n",
    "                # Reinitialize optimizer - now excluding the frozen parameters\n",
    "            fine_tune_optimizer = torch.optim.AdamW(\n",
    "                filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                # lr = learning_rate_fcn / 10  # 20, 6\n",
    "                lr = learning_rate_fcn * 0.1\n",
    "                # lr = learning_rate_fcn * 0.8\n",
    "            )\n",
    "\n",
    "            opt = fine_tune_optimizer \n",
    "            schedular = lr_scheduler.StepLR(opt, step_size=20, gamma=0.9)\n",
    "\n",
    "            # also, we don't consider the penalty when train since two Q are already frozen\n",
    "            \n",
    "            ## control_penalty_mse = 0\n",
    "            \n",
    "            # do we need to also set lambda to 1 and mute regis loss?\n",
    "            # to-be-done\n",
    "            \n",
    "            ## mac = 0\n",
    "            \n",
    "            # lamda = 1\n",
    "            early_stopping_firedown.counter = 0\n",
    "\n",
    "        ########################################\n",
    "        ########################################\n",
    "        best_val = early_stopping_firedown.best_loss\n",
    "        best_subLoss_val = early_stopping_firedown.best_sub_loss\n",
    "        ##################################################################################################\n",
    "        # wait, best model should be decided on acc or val_loss?\n",
    "        ##################################################################################################\n",
    "        # additional: best_regis_and_penal\n",
    "        # (omit classification)\n",
    "        if best_subLoss_val == subLoss_val[-1]:\n",
    "            # best_val = loss_val[-1]\n",
    "            best_model = model\n",
    "            best_train = loss_train[-1]\n",
    "            best_epoch = epoch\n",
    "            # Save the current best model\n",
    "            save_model(folder, \"best_subLoss\", Srvf_params, FF_params, model, opt)\n",
    "\n",
    "        # based on best valid loss\n",
    "        if best_val == loss_val[-1]:\n",
    "            # best_val = loss_val[-1]\n",
    "            best_model = model\n",
    "            best_train = loss_train[-1]\n",
    "            best_epoch = epoch\n",
    "            # Save the current best model\n",
    "            save_model(folder, \"best\", Srvf_params, FF_params, model, opt)\n",
    "        ####################################################################################################\n",
    "        # based on best valid acc\n",
    "        best_val_acc = early_stopping_firedown.best_acc\n",
    "\n",
    "        if best_val_acc == acc_val[-1]:\n",
    "            # best_val = loss_val[-1]\n",
    "            best_model_bacc = model\n",
    "            best_train_bacc = loss_train[-1]\n",
    "            best_epoch_bacc = epoch\n",
    "            # Save the current best model\n",
    "            save_model(folder, \"best_acc_\", Srvf_params, FF_params, model, opt)\n",
    "\n",
    "        ####################################################################################################\n",
    "\n",
    "        print(\"Epoch : {}\".format(epoch))\n",
    "        print(\"\\t Training loss : {:.5f}\".format(loss_train[-1]))\n",
    "        print(\"\\t Validation loss : {:.5f}\".format(loss_val[-1]))\n",
    "        print(\"\\t (train) Registration loss : {:.5f} \\t Classification loss : {:.5f}\".format(left_loss_train[-1], right_loss_train[-1]))\n",
    "        print(\"\\t (valid) Registration loss : {:.5f} \\t Classification loss : {:.5f}\".format(regis_loss_valid[-1], classify_loss_valid[-1]))\n",
    "        if not magic_token: # freezed\n",
    "            print(\"\\t penalty (mse) : [[{:.5f}]]\".format(penal_1_train[-1]))\n",
    "        else:\n",
    "            print(\"\\t penalty (mse) : {:.5f}\".format(penal_1_train[-1]))\n",
    "        print(\"\\t valid accuracy : {:.5f}\".format(acc_val[-1]))\n",
    "        # print(\"\\t best_subLoss_val{:.5f}\".format(best_subLoss_val))\n",
    "        # :.4f\n",
    "        # :.6E, scientific notation\n",
    "    time_elapsed = time.time() - since\n",
    "    print(\n",
    "        \"Training complete in {:.4f}s  {:.0f}m {:.0f}s\".format(\n",
    "            time_elapsed, time_elapsed // 60, time_elapsed % 60\n",
    "        )\n",
    "    )\n",
    "    # deep copy the model\n",
    "    # if phase == 'val' and loss_val[-1] > loss_train[-1]:\n",
    "    #        best_acc = epoch_acc\n",
    "    #        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    return (\n",
    "        best_model,\n",
    "        subLoss_val,\n",
    "        loss_val,\n",
    "        loss_train,\n",
    "        left_loss_train,\n",
    "        right_loss_train,\n",
    "        penal_1_train,\n",
    "        acc_val,\n",
    "        time_elapsed,\n",
    "        best_val,\n",
    "        best_train,\n",
    "        best_epoch,\n",
    "        regis_loss_valid,\n",
    "        regisA_loss_valid,\n",
    "        regisB_loss_valid,\n",
    "        regisA_loss_train,\n",
    "        regisB_loss_train,\n",
    "        classify_loss_valid,\n",
    "        Q_A_train_history,\n",
    "        Q_B_train_history,\n",
    "        Q_A_mean_train_history,\n",
    "        Q_B_mean_train_history,\n",
    "        Q_A_valid_history,\n",
    "        Q_B_valid_history,\n",
    "        Q_A_mean_valid_history,\n",
    "        Q_B_mean_valid_history\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bc132b",
   "metadata": {},
   "source": [
    "# Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff565047",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, subLoss_val, loss_val, loss_train, left_loss_train, right_loss_train, penal_1_train, acc_val, time_elapsed, best_val,best_train,best_epoch, regis_loss_valid, regisA_loss_valid, regisB_loss_valid, regisA_loss_train, regisB_loss_train, classify_loss_valid, Q_A_train_history, Q_B_train_history, Q_A_mean_train_history, Q_B_mean_train_history, Q_A_valid_history, Q_B_valid_history, Q_A_mean_valid_history, Q_B_mean_valid_history= train_model_trainval(model=model,batch_size=batch_size, num_epochs=num_epochs,data_train=processed_data_train, device=device,schedular = exp_lr_scheduler ,data_val=processed_data_valid.float() ,optimizer=optimizer, criterion=criterion, lamda=lamda, subLoss_val=[], loss_val=[],loss_train=[],left_loss_train=[],right_loss_train=[],penal_1_train=[], acc_val=[],regis_loss_valid=[], regisA_loss_valid=[], regisB_loss_valid=[], regisA_loss_train=[], regisB_loss_train=[], classify_loss_valid=[], Q_A_train_history=[], Q_B_train_history=[], Q_A_mean_train_history=[], Q_B_mean_train_history=[], Q_A_valid_history=[], Q_B_valid_history=[], Q_A_mean_valid_history=[], Q_B_mean_valid_history=[], visualize_Q_for_n_times=5, time=time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a424af60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(regisA_loss_valid), len(regisB_loss_valid), len(regisA_loss_train), len(regisB_loss_train)\n",
    "plt.figure(figsize=[8.0, 4.8])\n",
    "plt.plot(regisA_loss_valid,label='regis_loss_A (valid)')\n",
    "plt.plot(regisA_loss_train,label='regis_loss_A (train)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=[8.0, 4.8])\n",
    "plt.plot(regisB_loss_valid,label='regis_loss_B (valid)')\n",
    "plt.plot(regisB_loss_train,label='regis_loss_B (train)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1347e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8.0, 4.8])\n",
    "plt.plot(regis_loss_valid,label='regis_loss_valid')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=[8.0, 4.8])\n",
    "plt.plot(classify_loss_valid,label='classify_loss_valid')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=[8.0, 4.8])\n",
    "plt.plot(penal_1_train,label='penal_1 (mse)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0cbd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8.0, 4.8])\n",
    "plt.plot(loss_train,label='Training loss')\n",
    "plt.plot(left_loss_train,label='Registration loss')\n",
    "plt.plot(right_loss_train,label='Classification loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e050d1b",
   "metadata": {},
   "source": [
    "*How **Q** change during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be988195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tensor_array(tensor_array, seq_len, plot_color, title):\n",
    "  num_tensors = len(tensor_array)\n",
    "\n",
    "  for i in range(num_tensors):\n",
    "    if i==0: # start point\n",
    "      plt.plot(tensor_array[i].detach().cpu(), color=\"red\", label='start')\n",
    "    elif i == num_tensors-1:\n",
    "      plt.plot(tensor_array[i].detach().cpu(), color=\"green\", label='end')\n",
    "    else:\n",
    "      plt.plot(tensor_array[i].detach().cpu(), color=plot_color)  \n",
    "\n",
    "\n",
    "  plt.xlabel(\"X\")\n",
    "  plt.ylabel(\"Y\")\n",
    "  plt.legend(loc=\"upper left\")\n",
    "  plt.title(title)\n",
    "  plt.grid(True)  # Add grid for better readability\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "plot_tensor_array(Q_A_mean_train_history, seq_len, \"orange\", \"Q_A_train_history\")\n",
    "plot_tensor_array(Q_B_mean_train_history, seq_len, \"royalblue\", \"Q_B_train_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee18dd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot two classes together\n",
    "def How_Q_change_start_to_end(tensor_array_A, plot_color_A, tensor_array_B, plot_color_B, title):\n",
    "  # class_A\n",
    "  plt.plot(tensor_array_A[0].detach().cpu(), color=plot_color_A, linestyle='--') # start\n",
    "  plt.plot(tensor_array_A[-1].detach().cpu(), color=plot_color_A) # end\n",
    "\n",
    "  # class_B\n",
    "  plt.plot(tensor_array_B[0].detach().cpu(), color=plot_color_B, linestyle='--') # start\n",
    "  plt.plot(tensor_array_B[-1].detach().cpu(), color=plot_color_B) # end\n",
    "\n",
    "  plt.xlabel(\"X\")\n",
    "  plt.ylabel(\"Y\")\n",
    "  plt.title(title)\n",
    "  plt.grid(True)  # Add grid for better readability\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "  diff_dist_start = mse_criterion(tensor_array_A[0].cpu(), tensor_array_B[0].cpu())\n",
    "  # when diff_dist is smaller, the penalty should be larger\n",
    "  # so we use 1 / diff_dist\n",
    "  mse_dist_start = 0.1 / diff_dist_start\n",
    "  diff_dist_end = mse_criterion(tensor_array_A[-1].cpu(), tensor_array_B[-1].cpu())\n",
    "  mse_dist_end = 0.1 / diff_dist_end\n",
    "\n",
    "  print(f\"mse_dist: {mse_dist_start} -> {mse_dist_end}\")\n",
    "\n",
    "How_Q_change_start_to_end(Q_A_mean_train_history, \"orange\", Q_B_mean_train_history, \"royalblue\", \"changes for A and B (train)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f74c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for valid data\n",
    "How_Q_change_start_to_end(Q_A_mean_valid_history, \"orange\", Q_B_mean_valid_history, \"royalblue\", \"changes for A and B (valid)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c6147",
   "metadata": {},
   "source": [
    "# plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b3f22a",
   "metadata": {},
   "source": [
    "1. Fourier input correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e739e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_signal_train = pd.read_csv(\"data/simulation/\" + case + \"/train_true.csv\", header=None)\n",
    "true_signal_test = pd.read_csv(\"data/simulation/\" + case + \"/test_true.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1add0ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_signal_train_array = true_signal_train.values\n",
    "true_signal_test_array = true_signal_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12568ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_true_train_data = true_signal_train_array[_order_train]\n",
    "raw_true_test_data = true_signal_test_array[_order_test]\n",
    "\n",
    "# the last col is response variable\n",
    "data_true_train = raw_true_train_data[:,:-1]\n",
    "label_true_train = raw_true_train_data[:, [-1]]\n",
    "data_true_test = raw_true_test_data[:, :-1]\n",
    "label_true_test = raw_true_test_data[:, [-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643dd7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply normalization\n",
    "data_true_train = torch.from_numpy(data_true_train).unsqueeze(1)\n",
    "data_true_test = torch.from_numpy(data_true_test).unsqueeze(1)\n",
    "norm_data_true_train, _, _ = Normalization_on_data(data_true_train)\n",
    "norm_data_true_test, _, _ = Normalization_on_data(data_true_test)\n",
    "# Sanity check\n",
    "is_one = torch.isclose(norm_data_true_train.std(dim=2)[0][0], torch.tensor(1.0, dtype=norm_data_true_train.dtype), rtol=1e-05, atol=1e-08)\n",
    "if not is_one:\n",
    "    raise(\"ERROR in normalization, plz check!\")\n",
    "else:\n",
    "    print(\"Data is normalized...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82f1b7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_data_true_train = norm_data_true_train.squeeze(1).to(device)\n",
    "norm_data_true_test = norm_data_true_test.squeeze(1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "729207da",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_true_train = torch.from_numpy(label_true_train).unsqueeze(1)\n",
    "label_true_test = torch.from_numpy(label_true_test).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40331faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train_A = (label_true_train[:,-1] == 0.).nonzero(as_tuple=True)[0]\n",
    "idx_train_B = (label_true_train[:,-1] == 1.).nonzero(as_tuple=True)[0]\n",
    "idx_test_A = (label_true_test[:,-1] == 0.).nonzero(as_tuple=True)[0]\n",
    "idx_test_B = (label_true_test[:,-1] == 1.).nonzero(as_tuple=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7c94c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fourier = 50\n",
    "fourier_true_train_A = to_fourier_coefficients(norm_data_true_train[idx_train_A], n=n_fourier)\n",
    "fourier_true_train_B = to_fourier_coefficients(norm_data_true_train[idx_train_B], n=n_fourier)\n",
    "fourier_true_test_A = to_fourier_coefficients(norm_data_true_test[idx_test_A], n=n_fourier)\n",
    "fourier_true_test_B = to_fourier_coefficients(norm_data_true_test[idx_test_B], n=n_fourier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a80be53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_centralized_functions(history_idx, device, processed_data_train, label_train, n_fourier):\n",
    "    # load the best model\n",
    "    ck = folder + str(history_idx) +\"_checkpoint.pth\"\n",
    "    # ck = folder + \"best_acc__checkpoint.pth\" # the one with highest acc\n",
    "    model_eval = load_model(ck, device)\n",
    "    model_eval.eval()\n",
    "\n",
    "    # Obtain Centralized warping & warped functions\n",
    "    _, warped_training_data = Generate_centralized_warping_functions(processed_data_train, model_eval, r_mean_inverse = r_mean_inverse, device=device)\n",
    "\n",
    "    idx_A = (label_train[:,-1] == 0.).nonzero(as_tuple=True)[0]\n",
    "    idx_B = (label_train[:,-1] == 1.).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    warped_training_data_class_A = warped_training_data[idx_A]\n",
    "    warped_training_data_class_B = warped_training_data[idx_B]\n",
    "    fourier_warped_train_A = to_fourier_coefficients(warped_training_data_class_A, n=n_fourier)\n",
    "    fourier_warped_train_B = to_fourier_coefficients(warped_training_data_class_B, n=n_fourier)\n",
    "\n",
    "    return fourier_warped_train_A, fourier_warped_train_B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bead2058",
   "metadata": {},
   "outputs": [],
   "source": [
    "fourier_warped_train_A, fourier_warped_train_B = fourier_centralized_functions(1,device,processed_data_train, label_train, n_fourier = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9729bb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_coefficient(x, y):\n",
    "    x = x.cpu()\n",
    "    y = y.cpu()\n",
    "    x_np = x.numpy()\n",
    "    y_np = y.numpy()\n",
    "    cor = np.zeros(x_np.shape[0])\n",
    "    for i in range(x_np.shape[0]):\n",
    "        cor[i] = np.corrcoef(x_np[i], y_np[i])[0, 1]\n",
    "    return cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e55223",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_m = len(loss_train) - 1\n",
    "history_idx = [1,5,15,30,id_m,id_m]\n",
    "fig_size = len(history_idx)\n",
    "\n",
    "data = []\n",
    "for i in range(fig_size):\n",
    "    if i == 0:\n",
    "        idx_A = (label_train[:,-1] == 0.).nonzero(as_tuple=True)[0]\n",
    "        idx_B = (label_train[:,-1] == 1.).nonzero(as_tuple=True)[0]\n",
    "        start_A = norm_data_train[idx_A].squeeze(1)\n",
    "        start_B = norm_data_train[idx_B].squeeze(1)\n",
    "        start_A_f = to_fourier_coefficients(start_A, n=n_fourier)\n",
    "        start_B_f = to_fourier_coefficients(start_B, n=n_fourier)\n",
    "        cor_A = correlation_coefficient(start_A_f,fourier_true_train_A)\n",
    "        cor_B = correlation_coefficient(start_B_f,fourier_true_train_A)\n",
    "        cor = np.concatenate([cor_A,cor_B],axis = 0)\n",
    "        data.append([cor_A])\n",
    "    elif i == fig_size - 1:\n",
    "        fourier_A, fourier_B = fourier_centralized_functions(history_idx[i],device,processed_data_test, label_test, n_fourier = 50)\n",
    "        cor_A = correlation_coefficient(fourier_A,fourier_true_test_A)\n",
    "        cor_B = correlation_coefficient(fourier_B,fourier_true_test_B)\n",
    "        cor = np.concatenate([cor_A,cor_B],axis = 0)\n",
    "        data.append([cor_A])\n",
    "        \n",
    "    else: \n",
    "        fourier_A, fourier_B = fourier_centralized_functions(history_idx[i],device,processed_data_train, label_train, n_fourier = 50)\n",
    "        cor_A = correlation_coefficient(fourier_A,fourier_true_train_A)\n",
    "        cor_B = correlation_coefficient(fourier_B,fourier_true_train_B)\n",
    "        cor = np.concatenate([cor_A,cor_B],axis = 0)\n",
    "        data.append([cor_A])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "positions = np.arange(1, 7)\n",
    "width = 0.5\n",
    "\n",
    "colors = [\"orange\"]\n",
    "labels = ['Class A']\n",
    "for i in range(1):\n",
    "    boxplots = ax.boxplot(\n",
    "        [data[j][i] for j in range(6)],\n",
    "        positions=positions,\n",
    "        widths=width,\n",
    "        patch_artist=True\n",
    "    )\n",
    "    for patch in boxplots['boxes']:\n",
    "        patch.set_facecolor(colors[i])\n",
    "    for element in ['whiskers', 'caps', 'medians']:\n",
    "        plt.setp(boxplots[element], color=colors[i])\n",
    "\n",
    "\n",
    "ax.set_xticks(positions)\n",
    "ax.set_xticklabels([\"Train Start\",\"Train E\"+str(history_idx[1]*5),\"Train E\"+str(history_idx[2]*5),\"Train E\"+str(history_idx[3]*5),\"Train End\",\"Test\"])\n",
    "ax.set_ylabel('Correlation')\n",
    "ax.set_title('Correlation within true signals')\n",
    "ax.grid(axis = \"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9292c641",
   "metadata": {},
   "source": [
    "2. Distence in Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c42b6cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(f):\n",
    "    \"\"\"\n",
    "    Central difference method with equal space in dx(i.e.1)\n",
    "    Input : f in tensor (batch size x fun len)\n",
    "    Output : gradient along axis fun len(dim=1)\n",
    "\n",
    "    \"\"\"\n",
    "    row_num = f.ndim  # y.size()[0]\n",
    "    dx = torch.arange(f.size()[1])  # only 1d\n",
    "    output = torch.empty_like(f)\n",
    "    # i=0\n",
    "    output[:, 0] = torch.div(\n",
    "        f[:, 1] - f[:, 0], dx[1] - dx[0]\n",
    "    )  # y[:,1] - y[:,0]/ dx[1]-dx[0]\n",
    "    # i = end\n",
    "    output[:, -1] = torch.div(\n",
    "        f[:, -1] - f[:, -2], dx[-1] - dx[-2]\n",
    "    )  # y[:,-1] - y[:,-2]/  dx[-1]-dx[-2]\n",
    "    # i = 1:end-1\n",
    "    for i in range(1, len(dx) - 1):\n",
    "        output[:, i] = torch.div(f[:, i - 1] - f[:, i + 1], dx[i - 1] - dx[i + 1])\n",
    "\n",
    "    return output\n",
    "\n",
    "def srvf(f, gradient = gradient):\n",
    "    \"\"\"\n",
    "    v (batch size x fun len )\n",
    "    q = sign(f')* sqrt(|f'|)\n",
    "    No backward\n",
    "    \"\"\"\n",
    "    q = torch.empty_like(f)\n",
    "    batch, length = f.size()\n",
    "    length_adjust = length - 1\n",
    "    grad_f = gradient(f * length_adjust)\n",
    "    for i in range(batch):\n",
    "        q[i, :] = torch.sign(grad_f[i, :]) * torch.sqrt(grad_f[i, :].abs())\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f948f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_A = (label_train[:,-1] == 0.).nonzero(as_tuple=True)[0]\n",
    "idx_B = (label_train[:,-1] == 1.).nonzero(as_tuple=True)[0]\n",
    "processed_data_train_class_A = processed_data_train[idx_A,:,:-1].squeeze(1)\n",
    "processed_data_train_class_B = processed_data_train[idx_B,:,:-1].squeeze(1)\n",
    "processed_data_train_class_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c84b7cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_loss = []\n",
    "\n",
    "history_idx = [0,5,15,30,id_m,0,id_m,0]\n",
    "idx_A = (label_train[:,-1] == 0.).nonzero(as_tuple=True)[0]\n",
    "idx_B = (label_train[:,-1] == 1.).nonzero(as_tuple=True)[0]\n",
    "processed_data_train_class_A = processed_data_train[idx_A,:,:-1].squeeze(1)\n",
    "processed_data_train_class_B = processed_data_train[idx_B,:,:-1].squeeze(1)\n",
    "idx_A_test = (label_test[:,-1] == 0.).nonzero(as_tuple=True)[0]\n",
    "idx_B_test = (label_test[:,-1] == 1.).nonzero(as_tuple=True)[0]\n",
    "processed_data_test_class_A = processed_data_test[idx_A_test,:,:-1]\n",
    "processed_data_test_class_B = processed_data_test[idx_B_test,:,:-1]\n",
    "\n",
    "for i in range(len(history_idx)):\n",
    "    if i == 0:\n",
    "        Q_A = srvf(processed_data_train_class_A)\n",
    "        Q_B = srvf(processed_data_train_class_B)\n",
    "    elif i == len(history_idx) - 3:\n",
    "        true_A_train = norm_data_true_train[idx_train_A,:]\n",
    "        true_B_train = norm_data_true_train[idx_train_B,:]\n",
    "        Q_A = srvf(true_A_train)\n",
    "        Q_B = srvf(true_B_train)\n",
    "    elif i == len(history_idx) - 2:\n",
    "        ck = folder + str(history_idx[i]) +\"_checkpoint.pth\"\n",
    "        # ck = folder + \"best_acc__checkpoint.pth\" # the one with highest acc\n",
    "        model_eval = load_model(ck, device)\n",
    "        model_eval.eval()\n",
    "        _, Q_A, _, _ = model_eval(processed_data_test_class_A)\n",
    "        _, Q_B, _, _ = model_eval(processed_data_test_class_B)\n",
    "    elif i == len(history_idx) - 1:\n",
    "        true_A_test = norm_data_true_test[idx_test_A,:]\n",
    "        true_B_test = norm_data_true_test[idx_test_B,:]\n",
    "        Q_A = srvf(true_A_test)\n",
    "        Q_B = srvf(true_B_test)\n",
    "    else:\n",
    "        ck = folder + str(history_idx[i]) +\"_checkpoint.pth\"\n",
    "        # ck = folder + \"best_acc__checkpoint.pth\" # the one with highest acc\n",
    "        model_eval = load_model(ck, device)\n",
    "        model_eval.eval()\n",
    "        _, Q_A, _, _ = model_eval(processed_data_train_class_A.unsqueeze(1))\n",
    "        _, Q_B, _, _ = model_eval(processed_data_train_class_B.unsqueeze(1))\n",
    "\n",
    "    Q_mean_A_single = Q_A.mean(0)\n",
    "    Q_mean_A = Q_mean_A_single.repeat(Q_A.shape[0], 1)\n",
    "\n",
    "    # Q_mean_B_single = Q_B.mean(0).detach()\n",
    "    Q_mean_B_single = Q_B.mean(0)\n",
    "    Q_mean_B = Q_mean_B_single.repeat(Q_B.shape[0], 1)\n",
    "\n",
    "    left_loss_A = mse_criterion(Q_A, Q_mean_A)\n",
    "    left_loss_B = mse_criterion(Q_B, Q_mean_B)\n",
    "    left_loss = left_loss_A + left_loss_B\n",
    "    Q_loss.append(left_loss.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e4b807",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"Train Start\",\"Train E\"+str(history_idx[1]),\"Train E\"+str(history_idx[2]),\"Train E\"+str(history_idx[3]),\"Train End\",\"Train true\",\"Test\",\"Test true\"]\n",
    "values1 = Q_loss\n",
    "\n",
    "bar_width = 0.35\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "index = range(len(categories))\n",
    "plt.bar(index, values1, bar_width, label='Group 1', color=\"orange\")\n",
    "\n",
    "plt.title('Q loss for each epoch')\n",
    "plt.ylabel('Q loss')\n",
    "plt.xticks([i for i in index], categories)\n",
    "\n",
    "for i, v in enumerate(values1):\n",
    "    plt.text(i, v + 0.25, f\"{v:.3f}\", ha='center')\n",
    "    \n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756d4061",
   "metadata": {},
   "source": [
    "3. Plot TV for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e10240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TV_f = []\n",
    "\n",
    "history_idx = [0,5,15,30,id_m,0,id_m,0]\n",
    "idx_A = (label_train[:,-1] == 0.).nonzero(as_tuple=True)[0]\n",
    "idx_B = (label_train[:,-1] == 1.).nonzero(as_tuple=True)[0]\n",
    "processed_data_train_class_A = processed_data_train[idx_A,:,:-1].squeeze(1)\n",
    "processed_data_train_class_B = processed_data_train[idx_B,:,:-1].squeeze(1)\n",
    "idx_A_test = (label_test[:,-1] == 0.).nonzero(as_tuple=True)[0]\n",
    "idx_B_test = (label_test[:,-1] == 1.).nonzero(as_tuple=True)[0]\n",
    "processed_data_test_class_A = processed_data_test[idx_A_test,:,:-1].squeeze(1)\n",
    "processed_data_test_class_B = processed_data_test[idx_B_test,:,:-1].squeeze(1)\n",
    "\n",
    "for i in range(len(history_idx)):\n",
    "    TV = 0\n",
    "    if i == 0:\n",
    "        f_r_A = processed_data_train_class_A\n",
    "        f_r_B = processed_data_train_class_B\n",
    "    elif i == len(history_idx) - 3:\n",
    "        f_r_A = norm_data_true_train[idx_train_A,:]\n",
    "        f_r_B = norm_data_true_train[idx_train_B,:]\n",
    "    elif i == len(history_idx) - 2:\n",
    "        ck = folder + str(history_idx[i]) +\"_checkpoint.pth\"\n",
    "        # ck = folder + \"best_acc__checkpoint.pth\" # the one with highest acc\n",
    "        model_eval = load_model(ck, device)\n",
    "        model_eval.eval()\n",
    "        _, _, _, f_r_A = model_eval(processed_data_test_class_A.unsqueeze(1))\n",
    "        _, _, _, f_r_B = model_eval(processed_data_test_class_B.unsqueeze(1))\n",
    "    elif i == len(history_idx) - 1:\n",
    "        f_r_A = norm_data_true_test[idx_test_A,:]\n",
    "        f_r_B = norm_data_true_test[idx_test_B,:]\n",
    "    else:\n",
    "        ck = folder + str(history_idx[i]) +\"_checkpoint.pth\"\n",
    "        # ck = folder + \"best_acc__checkpoint.pth\" # the one with highest acc\n",
    "        model_eval = load_model(ck, device)\n",
    "        model_eval.eval()\n",
    "        _, _, _, f_r_A = model_eval(processed_data_train_class_A.unsqueeze(1))\n",
    "        _, _, _, f_r_B = model_eval(processed_data_train_class_B.unsqueeze(1))\n",
    "\n",
    "    for i in range(f_r_A.shape[0]):\n",
    "        TV += torch.sum(torch.pow(f_r_A[i] - torch.mean(f_r_A,axis = 0),2))\n",
    "    for i in range(f_r_B.shape[0]):\n",
    "        TV += torch.sum(torch.pow(f_r_B[i] - torch.mean(f_r_B,axis = 0),2))\n",
    "    TV /= 2 * f_r_A.shape[0]\n",
    "    \n",
    "    TV_f.append(TV.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd950a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"Train Start\",\"Train E\"+str(history_idx[1]),\"Train E\"+str(history_idx[2]),\"Train E\"+str(history_idx[3]),\"Train End\",\"Train true\",\"Test\",\"Test true\"]\n",
    "values1 = TV_f\n",
    "\n",
    "bar_width = 0.35\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "index = range(len(categories))\n",
    "plt.bar(index, values1, bar_width, label='Group 1', color=\"orange\")\n",
    "\n",
    "plt.title('TV for each epoch')\n",
    "plt.ylabel('TV')\n",
    "plt.xticks([i for i in index], categories)\n",
    "\n",
    "for i, v in enumerate(values1):\n",
    "    plt.text(i, v + 0.25, f\"{v:.3f}\", ha='center')\n",
    "    \n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7b30d3",
   "metadata": {},
   "source": [
    "4. Centered training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f782e51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_signal_0 = pd.read_csv(\"data/simulation/row_data/allTrueSignals_0.csv\", header=None)\n",
    "true_signal_1 = pd.read_csv(\"data/simulation/row_data/allTrueSignals_1.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50398904",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_signal_0_array = true_signal_0.values\n",
    "true_signal_1_array = true_signal_1.values\n",
    "true_signal_0_array.shape, true_signal_1_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9255153e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalization_on_data(input_f):\n",
    "    '''\n",
    "    input_f N,C=1,L : () torch.tensor\n",
    "    Output N,C=1,L: After Z normalization\n",
    "    '''\n",
    "    length = input_f.size()[2]\n",
    "    miu = torch.mean(input_f,dim=2).repeat(1,length).unsqueeze(1)    \n",
    "    input_f = input_f - miu\n",
    "    sigma = input_f.std(dim=2,keepdim=True)    \n",
    "    input_f = input_f / sigma\n",
    "    return input_f, miu, sigma\n",
    "\n",
    "# apply normalization\n",
    "true_signal_0_array = torch.from_numpy(true_signal_0_array).unsqueeze(1)\n",
    "norm_true_0, miu_0, sigma_0 = Normalization_on_data(true_signal_0_array)\n",
    "\n",
    "true_signal_1_array = torch.from_numpy(true_signal_1_array).unsqueeze(1)\n",
    "norm_true_1, miu_1, sigma_1 = Normalization_on_data(true_signal_1_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1676533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "\n",
    "normed_true_signal_0_array = norm_true_0.squeeze().numpy()\n",
    "\n",
    "normed_true_signal_1_array = norm_true_1.squeeze().numpy()\n",
    "\n",
    "normed_true_signal_0_array.shape, normed_true_signal_1_array.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "04a6ed6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralized_functions(history_idx, device, processed_data_train, label_train):\n",
    "    # load the best model\n",
    "    ck = folder + str(history_idx) +\"_checkpoint.pth\"\n",
    "    # ck = folder + \"best_acc__checkpoint.pth\" # the one with highest acc\n",
    "    model_eval = load_model(ck, device)\n",
    "    model_eval.eval()\n",
    "\n",
    "    # Obtain Centralized warping & warped functions\n",
    "    r_mean_train_inv, warped_training_data = Generate_centralized_warping_functions(processed_data_train, model_eval, r_mean_inverse = r_mean_inverse, device=device)\n",
    "\n",
    "    idx_A = (label_train[:,-1] == 0.).nonzero(as_tuple=True)[0]\n",
    "    idx_B = (label_train[:,-1] == 1.).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    warped_training_data_class_A = warped_training_data[idx_A,:-1]\n",
    "    warped_training_data_class_B = warped_training_data[idx_B,:-1]\n",
    "\n",
    "    return warped_training_data_class_A, warped_training_data_class_B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f501cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def How_f_r_change_start_to_end(processed_data, labels, plot_color_A, plot_color_B, \n",
    "                                true_signal_0_array, true_signal_1_array, title, history_idx = [1,5,15,30,id_m], alpha = 0.2):\n",
    "\n",
    "    # Prepare the figure and subplots\n",
    "    fig, axes = plt.subplots(1, 6, figsize=(30, 5))  \n",
    "\n",
    "    # Titles for subplots\n",
    "    subplot_titles = [\"Signals train Start\", \"Signals train E5\", \"Signals train E15\", \"Signals train E30\", \"Signals train End\", \"True Signals\"]\n",
    "\n",
    "    processed_data_here = processed_data\n",
    "    label_here = labels\n",
    "\n",
    "\n",
    "    # Iterate through subplots for Class A\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i == 0:\n",
    "            warped_training_data_class_A, _ = centralized_functions(history_idx[i], device, processed_data_here, label_here)\n",
    "            for curve in (label_train[:,-1] == 0.).nonzero(as_tuple=True)[0]:\n",
    "                ax.plot(norm_data_train[curve, 0, :], color=plot_color_A, linestyle='solid', label='Start', alpha=alpha) \n",
    "        elif i == 1:\n",
    "            warped_training_data_class_A, _ = centralized_functions(history_idx[i], device, processed_data_here, label_here)\n",
    "            for curve in range(warped_training_data_class_A.shape[0]):\n",
    "                ax.plot(warped_training_data_class_A[curve, :], color=plot_color_A, linestyle='solid', label='E'+str(history_idx[i]),alpha=alpha)\n",
    "        elif i == 2:\n",
    "            warped_training_data_class_A, _ = centralized_functions(history_idx[i], device, processed_data_here, label_here)\n",
    "            for curve in range(warped_training_data_class_A.shape[0]):\n",
    "                ax.plot(warped_training_data_class_A[curve, :], color=plot_color_A, linestyle='solid', label='E'+str(history_idx[i]),alpha=alpha)\n",
    "        elif i == 3:\n",
    "            warped_training_data_class_A, _ = centralized_functions(history_idx[i], device, processed_data_here, label_here)\n",
    "            for curve in range(warped_training_data_class_A.shape[0]):\n",
    "                ax.plot(warped_training_data_class_A[curve, :], color=plot_color_A, linestyle='solid', label='E'+str(history_idx[i]),alpha=alpha)         \n",
    "        elif i == 4:\n",
    "            warped_training_data_class_A, _ = centralized_functions(history_idx[i], device, processed_data_here, label_here)\n",
    "            for curve in range(warped_training_data_class_A.shape[0]):\n",
    "                ax.plot(warped_training_data_class_A[curve, :], color=plot_color_A, linestyle='solid', label='End',alpha=alpha) \n",
    "        elif i == 5:\n",
    "            for curve in range(true_signal_0_array.shape[0]):\n",
    "                ax.plot(true_signal_0_array[curve], color=plot_color_A, linestyle='solid', label='True Signal A')\n",
    "\n",
    "        ax.set_title(f\"Class A: {subplot_titles[i]}\")\n",
    "        ax.set_xlabel(\"X\")\n",
    "        ax.set_ylabel(\"Y\")\n",
    "        # ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Iterate through subplots for Class B\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i == 0:\n",
    "            _, warped_training_data_class_B = centralized_functions(history_idx[i], device, processed_data_here, label_here)\n",
    "            for curve in (label_train[:,-1] == 1.).nonzero(as_tuple=True)[0]:\n",
    "                ax.plot(norm_data_train[curve,0, :], color=plot_color_B, linestyle='solid', label='Start',alpha=alpha) \n",
    "        elif i == 1:\n",
    "            _, warped_training_data_class_B = centralized_functions(history_idx[i], device, processed_data_here, label_here)\n",
    "            for curve in range(warped_training_data_class_B.shape[0]):\n",
    "                ax.plot(warped_training_data_class_B[curve, :], color=plot_color_B, linestyle='solid', label='E'+str(history_idx[i]),alpha=alpha)\n",
    "        elif i == 2:\n",
    "            _, warped_training_data_class_B = centralized_functions(history_idx[i], device, processed_data_here, label_here)\n",
    "            for curve in range(warped_training_data_class_B.shape[0]):\n",
    "                ax.plot(warped_training_data_class_B[curve, :], color=plot_color_B, linestyle='solid', label='E'+str(history_idx[i]),alpha=alpha)\n",
    "        elif i == 3:\n",
    "            _, warped_training_data_class_B = centralized_functions(history_idx[i], device, processed_data_here, label_here)\n",
    "            for curve in range(warped_training_data_class_B.shape[0]):\n",
    "                ax.plot(warped_training_data_class_B[curve, :], color=plot_color_B, linestyle='solid', label='E'+str(history_idx[i]),alpha=alpha)\n",
    "        elif i == 4:\n",
    "            _, warped_training_data_class_B = centralized_functions(history_idx[i], device, processed_data_here, label_here)\n",
    "            for curve in range(warped_training_data_class_B.shape[0]):\n",
    "                ax.plot(warped_training_data_class_B[curve, :], color=plot_color_B, linestyle='solid', label='End',alpha=alpha) \n",
    "        elif i == 5:\n",
    "            for curve in range(true_signal_1_array.shape[0]):\n",
    "                ax.plot(true_signal_1_array[curve], color='blue', linestyle='solid', label='True Signal B') \n",
    "        \n",
    "        ax.set_title(f\"Class Both: {subplot_titles[i]}\")\n",
    "        ax.set_xlabel(\"X\")\n",
    "        ax.set_ylabel(\"Y\")\n",
    "        # ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Set the main title and adjust layout\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit the main title\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "62aae135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def How_f_r_change_start_to_end_test(processed_data, labels, plot_color_A, plot_color_B, \n",
    "                                true_signal_0_array, true_signal_1_array, title, history_idx = [1,id_m,1], alpha = 0.2):\n",
    "\n",
    "    # Prepare the figure and subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 5))  \n",
    "\n",
    "    # Titles for subplots\n",
    "    subplot_titles = [\"Signals test Start\", \"Signals test End\", \"True Signals\"]\n",
    "\n",
    "    processed_data_here = processed_data\n",
    "    label_here = labels\n",
    "\n",
    "\n",
    "    # Iterate through subplots for Class A\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i == 0:\n",
    "            warped_training_data_class_A, _ = centralized_functions(history_idx[i], device, processed_data_here, label_here)\n",
    "            for curve in (label_train[:,-1] == 0.).nonzero(as_tuple=True)[0]:\n",
    "                ax.plot(norm_data_test[curve, 0, :], color=plot_color_A, linestyle='solid', label='Start', alpha=alpha) \n",
    "        elif i == 1:\n",
    "            warped_training_data_class_A, _ = centralized_functions(history_idx[i], device, processed_data_here, label_here)\n",
    "            for curve in range(warped_training_data_class_A.shape[0]):\n",
    "                ax.plot(warped_training_data_class_A[curve, :], color=plot_color_A, linestyle='solid', label='E'+str(history_idx[i]),alpha=alpha)\n",
    "        elif i == 2:\n",
    "            for curve in range(true_signal_0_array.shape[0]):\n",
    "                ax.plot(true_signal_0_array[curve], color=plot_color_A, linestyle='solid', label='True Signal A')\n",
    "\n",
    "        ax.set_title(f\"Class A: {subplot_titles[i]}\")\n",
    "        ax.set_xlabel(\"X\")\n",
    "        ax.set_ylabel(\"Y\")\n",
    "        # ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Iterate through subplots for Class B\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i == 0:\n",
    "            _, warped_training_data_class_B = centralized_functions(history_idx[i], device, processed_data_here, label_here)\n",
    "            for curve in (label_train[:,-1] == 1.).nonzero(as_tuple=True)[0]:\n",
    "                ax.plot(norm_data_test[curve,0, :], color=plot_color_B, linestyle='solid', label='Start',alpha=alpha) \n",
    "        elif i == 1:\n",
    "            _, warped_training_data_class_B = centralized_functions(history_idx[i], device, processed_data_here, label_here)\n",
    "            for curve in range(warped_training_data_class_B.shape[0]):\n",
    "                ax.plot(warped_training_data_class_B[curve, :], color=plot_color_B, linestyle='solid', label='E'+str(history_idx[i]),alpha=alpha)\n",
    "        elif i == 2:\n",
    "            for curve in range(true_signal_1_array.shape[0]):\n",
    "                ax.plot(true_signal_1_array[curve], color=plot_color_B, linestyle='solid', label='True Signal B') \n",
    "        \n",
    "        ax.set_title(f\"Class Both: {subplot_titles[i]}\")\n",
    "        ax.set_xlabel(\"X\")\n",
    "        ax.set_ylabel(\"Y\")\n",
    "        # ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Set the main title and adjust layout\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit the main title\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3e127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "How_f_r_change_start_to_end_test(processed_data_test, label_test, \"orange\", \"royalblue\", normed_true_signal_0_array, normed_true_signal_1_array, \"Changes for A and B during training (f_r centered)\", alpha = 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90598a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "How_f_r_change_start_to_end(processed_data_train, label_train, \"orange\", \"royalblue\", normed_true_signal_0_array, normed_true_signal_1_array, \"Changes for A and B during training (f_r centered)\", alpha = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a8741f",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3006450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model\n",
    "ck = folder + \"best_checkpoint.pth\"\n",
    "# ck = folder + \"best_acc__checkpoint.pth\" # the one with highest acc\n",
    "model_eval = load_model(ck, device)\n",
    "model_eval.eval()\n",
    "\n",
    "# Obtain Centralized warping & warped functions\n",
    "r_cen_train, warped_training_data = Generate_centralized_warping_functions(processed_data_train, model_eval, r_mean_inverse = r_mean_inverse, device=device)\n",
    "r_cen_test, warped_test_data = Generate_centralized_warping_functions(processed_data_test, model_eval, r_mean_inverse = r_mean_inverse, device=device)\n",
    "\n",
    "# input data (before warping)\n",
    "tr_data = processed_data_train[:, :, :-1]\n",
    "te_data = processed_data_test[:, :, :-1]\n",
    "tr_label = processed_data_train[:, :, -1]\n",
    "te_label = processed_data_test[:, :, -1]\n",
    "input_training_data = tr_data.cpu()\n",
    "input_test_data = te_data.cpu()\n",
    "r_train = r_cen_train\n",
    "r_test = r_cen_test\n",
    "data_name = 'Multi-modal Data'\n",
    "\n",
    "# plot_warped_data_analysis(tr_label, te_label, warped_training_data, warped_test_data, input_training_data, input_test_data,\n",
    "#                               data_name, r_train, r_test, compute_signal_statistics=compute_signal_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aace78d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_signal_statistics(input_data):\n",
    "    input_data_mean = input_data[:, 0, :].mean(dim=0)\n",
    "    input_data_std = input_data[:, 0, :].std(dim=0)\n",
    "    lower = input_data_mean - input_data_std\n",
    "    upper = input_data_mean + input_data_std\n",
    "    criterion = nn.MSELoss(reduction=\"mean\")\n",
    "    loss = criterion(\n",
    "        input_data[:, 0, :], input_data_mean.repeat(input_data.shape[0], 1).detach()\n",
    "    )\n",
    "    return (\n",
    "        input_data_mean.cpu().detach(),\n",
    "        input_data_std.cpu().detach(),\n",
    "        lower.cpu().detach(),\n",
    "        upper.cpu().detach(),\n",
    "        loss.cpu().detach(),\n",
    "    )\n",
    "\n",
    "te_label = te_label.squeeze(1)\n",
    "te_label = te_label.cpu().numpy()\n",
    "# Get the within class var for each class\n",
    "warped_seq_class0 = warped_test_data[te_label == 0]\n",
    "warped_seq_class1 = warped_test_data[te_label == 1]\n",
    "\n",
    "num_test = te_label.shape[0]\n",
    "\n",
    "# visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(warped_seq_class0.shape[0]): \n",
    "    plt.plot(warped_seq_class0[i], color='orange')\n",
    "    \n",
    "plt.xlabel('Time points')\n",
    "plt.ylabel('sequence_warped values')\n",
    "plt.title(f'warped_sequence (class 0)')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(warped_seq_class1.shape[0]): \n",
    "    plt.plot(warped_seq_class1[i], color='royalblue')\n",
    "    \n",
    "plt.xlabel('Time points')\n",
    "plt.ylabel('sequence_warped values')\n",
    "plt.title(f'warped_sequence (class 1)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ea161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[32,6])\n",
    "fig.suptitle('SrvfRegNet')\n",
    "plt.subplot(1,2,1)\n",
    "for i in range(te_data.size(0)):\n",
    "    # plot two classes in different color\n",
    "    if te_label[i] == 0:\n",
    "        plt.plot(te_data.cpu()[i, 0, :], \"orange\")\n",
    "    else:\n",
    "        plt.plot(te_data.cpu()[i, 0, :], \"royalblue\")\n",
    "plt.title('Unaligned data')\n",
    "    \n",
    "plt.subplot(1,2,2)\n",
    "for i in range(warped_test_data.size(0)):\n",
    "    if te_label[i] == 0:\n",
    "        plt.plot(warped_test_data.cpu()[i, :], \"orange\")\n",
    "    else:\n",
    "        plt.plot(warped_test_data.cpu()[i, :], \"royalblue\")\n",
    "plt.title('Warped data')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e806f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class0 = warped_seq_class0.shape[0]\n",
    "class1 = warped_seq_class1.shape[0]\n",
    "\n",
    "random_order = np.random.permutation(class0 + class1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i in random_order:\n",
    "    if i < class0:\n",
    "        plt.plot(warped_seq_class0[i], color='orange')  \n",
    "    else:\n",
    "        plt.plot(warped_seq_class1[i - class0], color='royalblue')\n",
    "\n",
    "plt.xlabel('Time points')\n",
    "plt.ylabel('sequence_warped values')\n",
    "plt.title(f'warped_sequence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a7f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[12,6])\n",
    "for i in range(te_data.size(0)):\n",
    "    # plot two classes in different color\n",
    "    if te_label[i] == 0:\n",
    "        plt.plot(te_data.cpu()[i, 0, :], \"orange\")\n",
    "    else:\n",
    "        plt.plot(te_data.cpu()[i, 0, :], \"royalblue\")\n",
    "\n",
    "plt.xlabel('Time points')\n",
    "plt.ylabel('sequence_warped values')\n",
    "plt.title('Unaligned data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3495a338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print how wcv change after registration for each class\n",
    "# before warp\n",
    "seq_class0 = input_test_data[te_label == 0]\n",
    "seq_class1 = input_test_data[te_label == 1]\n",
    "\n",
    "(\n",
    "    test_mean_class0,\n",
    "    test_std_class0,\n",
    "    test_lower_class0,\n",
    "    test_upper_class0,\n",
    "    test_loss_class0,\n",
    ") = compute_signal_statistics(seq_class0)\n",
    "\n",
    "(\n",
    "    test_mean_class1,\n",
    "    test_std_class1,\n",
    "    test_lower_class1,\n",
    "    test_upper_class1,\n",
    "    test_loss_class1,\n",
    ") = compute_signal_statistics(seq_class1)\n",
    "\n",
    "# after warping\n",
    "(\n",
    "    test_warped_mean_class0,\n",
    "    test_warped_std_class0,\n",
    "    test_warped_lower_class0,\n",
    "    test_warped_upper_class0,\n",
    "    test_warped_loss_class0,\n",
    ") = compute_signal_statistics(warped_seq_class0.unsqueeze(1))\n",
    "\n",
    "(\n",
    "    test_warped_mean_class1,\n",
    "    test_warped_std_class1,\n",
    "    test_warped_lower_class1,\n",
    "    test_warped_upper_class1,\n",
    "    test_warped_loss_class1,\n",
    ") = compute_signal_statistics(warped_seq_class1.unsqueeze(1))\n",
    "\n",
    "print(f\"Class 0 (orange): {test_loss_class0} -> {test_warped_loss_class0}\")\n",
    "print(f\"Class 1 (blue): {test_loss_class1} -> {test_warped_loss_class1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae204105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy\n",
    "print(f\"we have {processed_data_test.shape[0]} test examples\")\n",
    "with torch.no_grad():\n",
    "    model_eval.eval()\n",
    "    y_bar, Q, r_test, f_r = model_eval(te_data)\n",
    "\n",
    "_, predicted = torch.max(y_bar, 1)\n",
    "te_label_ts = torch.as_tensor(te_label)\n",
    "test_loss = criterion(y_bar.cpu(), te_label_ts.long().cpu())\n",
    "num_correct = (predicted.cpu() == te_label_ts).sum().item()\n",
    "acc = num_correct / te_data.shape[0]\n",
    "print(\"test acc: \", acc)\n",
    "print(\"test loss: \", test_loss.item())\n",
    "f_r = f_r.shape\n",
    "print(f\"shape of f_r: {f_r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4378b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r_test.shape)\n",
    "r_test_np = r_test.cpu().numpy()\n",
    "len_seq=r_test_np.shape[1]\n",
    "print(len_seq)\n",
    "r_mean_test= r_test.mean(0).cpu().numpy()\n",
    "r_mean_test.shape\n",
    "x_index=np.arange(len_seq)\n",
    "r_mean_inverse_test=r_mean_inverse(x_index,r_mean_test,len_seq)\n",
    "r_mean_inverse_test.shape\n",
    "r_test_use=r_test.cpu().detach()\n",
    "r_cen_test=torch.zeros_like(r_test_use)\n",
    "for i in range(r_test_use.shape[0]):\n",
    "    r_cen_test[i,:]=torch.from_numpy(np.interp(r_mean_inverse_test,x_index,r_test_use[i,:]))\n",
    "r_cen_test.shape\n",
    "r_cen_test_np=r_cen_test.cpu().numpy()\n",
    "for i in range(r_cen_test_np.shape[0]):\n",
    "    if predicted[i]==0:\n",
    "        plt.plot(r_cen_test_np[i,:],color='orange')\n",
    "    else:\n",
    "        plt.plot(r_cen_test_np[i,:],color='royalblue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9026ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predicted.cpu().numpy()\n",
    "y_true = te_label_ts.cpu().numpy()\n",
    "\n",
    "def calculate_confusion_matrix_elements(y_true, y_pred):\n",
    "\n",
    "    tp = fp = tn = fn = 0\n",
    "    \n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if true == 1 and pred == 1:\n",
    "            tp += 1  # True positive\n",
    "        elif true == 1 and pred == 0:\n",
    "            fn += 1  # False nagetive\n",
    "        elif true == 0 and pred == 1:\n",
    "            fp += 1  # False positive\n",
    "        elif true == 0 and pred == 0:\n",
    "            tn += 1  # True nagetive\n",
    "            \n",
    "    return tp, fn, fp, tn\n",
    "\n",
    "\n",
    "tp, fn, fp, tn = calculate_confusion_matrix_elements(y_true, y_pred)\n",
    "\n",
    "print(f\"TP: {tp}, FN: {fn}, FP: {fp}, TN: {tn}\")\n",
    "\n",
    "\n",
    "\n",
    "def calculate_tpr(tp, fn):\n",
    "    return tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "\n",
    "def calculate_tnr(tn, fp):\n",
    "    return tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "\n",
    "def calculate_f1_score(tp, fp, fn):\n",
    "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    return 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "tpr = calculate_tpr(tp, fn)\n",
    "tnr = calculate_tnr(tn, fp)\n",
    "f1_score = calculate_f1_score(tp, fp, fn)\n",
    "\n",
    "tpr, tnr, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2593c61c",
   "metadata": {},
   "source": [
    "# Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9166c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cor(processed_data_train, label_train, fourier_true_train_A, fourier_true_train_B, model_eval, device = device, n_fourier = 50):\n",
    "    \n",
    "    _, warped_training_data = Generate_centralized_warping_functions(processed_data_train, model_eval, r_mean_inverse = r_mean_inverse, device=device)\n",
    "\n",
    "    idx_A = (label_train[:,-1] == 0.).nonzero(as_tuple=True)[0]\n",
    "    idx_B = (label_train[:,-1] == 1.).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    warped_training_data_class_A = warped_training_data[idx_A]\n",
    "    warped_training_data_class_B = warped_training_data[idx_B]\n",
    "    fourier_A = to_fourier_coefficients(warped_training_data_class_A, n=n_fourier)\n",
    "    fourier_B = to_fourier_coefficients(warped_training_data_class_B, n=n_fourier)\n",
    "\n",
    "    cor_A = correlation_coefficient(fourier_A,fourier_true_train_A)\n",
    "    cor_B = correlation_coefficient(fourier_B,fourier_true_train_B)\n",
    "    cor = np.concatenate([cor_A,cor_B],axis = 0)\n",
    "    return np.mean(cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "83a30dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_acc_f1(y_bar, label):\n",
    "    _, predicted = torch.max(y_bar, 1)\n",
    "    label_ts = torch.as_tensor(label)\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    y_true = label_ts.cpu().numpy()\n",
    "    \n",
    "    acc = accuracy_score(y_pred, y_true)\n",
    "    tp, fn, fp, _ = calculate_confusion_matrix_elements(y_true, y_pred)\n",
    "    f1_score = calculate_f1_score(tp, fp, fn)\n",
    "    \n",
    "    return acc, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fff8b7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Q_loss(Q_A, Q_B):\n",
    "    Q_mean_A_single = Q_A.mean(0)\n",
    "    Q_mean_A = Q_mean_A_single.repeat(Q_A.shape[0], 1)\n",
    "\n",
    "    # Q_mean_B_single = Q_B.mean(0).detach()\n",
    "    Q_mean_B_single = Q_B.mean(0)\n",
    "    Q_mean_B = Q_mean_B_single.repeat(Q_B.shape[0], 1)\n",
    "\n",
    "    left_loss_A = mse_criterion(Q_A, Q_mean_A)\n",
    "    left_loss_B = mse_criterion(Q_B, Q_mean_B)\n",
    "    left_loss = left_loss_A + left_loss_B\n",
    "    return left_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f9aa3e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_TV(f_r_A, f_r_B):\n",
    "    TV = 0 \n",
    "    for i in range(f_r_A.shape[0]):\n",
    "        TV += torch.sum(torch.pow(f_r_A[i] - torch.mean(f_r_A,axis = 0),2))\n",
    "    for i in range(f_r_B.shape[0]):\n",
    "        TV += torch.sum(torch.pow(f_r_B[i] - torch.mean(f_r_B,axis = 0),2))\n",
    "    TV /= f_r_A.shape[0] + f_r_B.shape[0]\n",
    "    \n",
    "    return TV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9f54f4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_A = (label_train[:,-1] == 0.).nonzero(as_tuple=True)[0]\n",
    "idx_B = (label_train[:,-1] == 1.).nonzero(as_tuple=True)[0]\n",
    "processed_data_train_class_A = processed_data_train[idx_A,:,:-1]\n",
    "processed_data_train_class_B = processed_data_train[idx_B,:,:-1]\n",
    "idx_A_test = (label_test[:,-1] == 0.).nonzero(as_tuple=True)[0]\n",
    "idx_B_test = (label_test[:,-1] == 1.).nonzero(as_tuple=True)[0]\n",
    "processed_data_test_class_A = processed_data_test[idx_A_test,:,:-1]\n",
    "processed_data_test_class_B = processed_data_test[idx_B_test,:,:-1]\n",
    "\n",
    "def compute_variable(history_id):\n",
    "    ck = folder + str(history_id) + \"_checkpoint.pth\"\n",
    "    model_eval = load_model(ck, device)\n",
    "    model_eval.eval()\n",
    "    y_bar, _, _, _ = model_eval(tr_data)\n",
    "    _, Q_A, _, f_r_A = model_eval(processed_data_train_class_A)\n",
    "    _, Q_B, _, f_r_B = model_eval(processed_data_train_class_B)\n",
    "    acc, f1 = compute_acc_f1(y_bar, tr_label)\n",
    "    TV = compute_TV(f_r_A, f_r_B)\n",
    "    Q_loss = compute_Q_loss(Q_A, Q_B)\n",
    "    cor = compute_cor(processed_data_train, label_train, fourier_true_train_A, fourier_true_train_B, model_eval)\n",
    "    del _, model_eval, y_bar, Q_A, Q_B, f_r_A, f_r_B \n",
    "     \n",
    "    return acc, f1, TV, Q_loss, cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ae9725",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "train_F1 = []\n",
    "train_TV = []\n",
    "train_Q_loss = []\n",
    "train_cor = []\n",
    "\n",
    "n_max = len(regisA_loss_valid)\n",
    "for i in range(n_max - 1):\n",
    "    j = i + 1\n",
    "    acc, f1, TV, Q_loss, cor = compute_variable(j)\n",
    "    train_acc.append(acc)\n",
    "    train_F1.append(f1)\n",
    "    train_TV.append(TV)\n",
    "    train_Q_loss.append(Q_loss)\n",
    "    train_cor.append(cor)\n",
    "    print(str(j) + \"/\" + str(n_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4659806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if cuda out of memory\n",
    "\n",
    "result_path = 'Result/simulation_result'\n",
    "if not os.path.exists(result_path):\n",
    "    os.makedirs(result_path)\n",
    "\n",
    "file_path = os.path.join(result_path, 'processed_data_train.csv')\n",
    "np.savetxt(file_path, processed_data_train.squeeze().cpu().numpy(), delimiter=',')\n",
    "file_path = os.path.join(result_path, 'processed_data_test.csv')\n",
    "np.savetxt(file_path, processed_data_test.squeeze().cpu().numpy(), delimiter=',')\n",
    "file_path = os.path.join(result_path, 'fourier_true_train_A.csv')\n",
    "np.savetxt(file_path, fourier_true_train_A.cpu().numpy(), delimiter=',')\n",
    "file_path = os.path.join(result_path, 'fourier_true_train_B.csv')\n",
    "np.savetxt(file_path, fourier_true_train_B.cpu().numpy(), delimiter=',')\n",
    "file_path = os.path.join(result_path, 'norm_data_train.csv')\n",
    "np.savetxt(file_path, norm_data_train.squeeze().cpu().numpy(), delimiter=',')\n",
    "file_path = os.path.join(result_path, 'norm_data_test.csv')\n",
    "np.savetxt(file_path, norm_data_test.squeeze().cpu().numpy(), delimiter=',')\n",
    "file_path = os.path.join(result_path, 'loss_train.csv')\n",
    "np.savetxt(file_path, np.array(loss_train), delimiter=',')\n",
    "file_path = os.path.join(result_path, 'left_loss_train.csv')\n",
    "np.savetxt(file_path, np.array(left_loss_train), delimiter=',')\n",
    "file_path = os.path.join(result_path, 'right_loss_train.csv')\n",
    "np.savetxt(file_path, np.array(right_loss_train), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "181c3a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data = processed_data_train[:, :, :-1]\n",
    "te_data = processed_data_test[:, :, :-1]\n",
    "tr_label = processed_data_train[:, :, -1]\n",
    "te_label = processed_data_test[:, :, -1]\n",
    "idx_A = (te_label[:,-1] == 0.).nonzero(as_tuple=True)[0]\n",
    "idx_B = (te_label[:,-1] == 1.).nonzero(as_tuple=True)[0]\n",
    "processed_data_test_class_A = processed_data_test[idx_A,:,:-1]\n",
    "processed_data_test_class_B = processed_data_test[idx_B,:,:-1]\n",
    "\n",
    "test_acc = 0\n",
    "test_F1 = 0\n",
    "test_TV = 0\n",
    "test_Q_loss = 0\n",
    "\n",
    "ck = folder + \"best_checkpoint.pth\"\n",
    "model_eval = load_model(ck, device)\n",
    "model_eval.eval()\n",
    "\n",
    "y_bar, _, _, _ = model_eval(te_data)\n",
    "_, Q_A, _, f_r_A = model_eval(processed_data_test_class_A)\n",
    "_, Q_B, _, f_r_B = model_eval(processed_data_test_class_B)\n",
    "\n",
    "test_acc, test_F1 = compute_acc_f1(y_bar, te_label)\n",
    "test_TV = compute_TV(f_r_A, f_r_B).cpu().detach().numpy()\n",
    "test_Q_loss = compute_Q_loss(Q_A, Q_B).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9f3282bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralized_functions_plot(history_idx, device, processed_data_train, best = False):\n",
    "    # load the best model\n",
    "    if best:\n",
    "        ck = folder + \"best_checkpoint.pth\"\n",
    "    else:\n",
    "        ck = folder + str(history_idx) +\"_checkpoint.pth\"\n",
    "    \n",
    "    model_eval = load_model(ck, device)\n",
    "    model_eval.eval()\n",
    "    # Obtain Centralized warping & warped functions\n",
    "    _, warped_training_data = Generate_centralized_warping_functions(processed_data_train, model_eval, r_mean_inverse = r_mean_inverse, device=device)\n",
    "\n",
    "    return warped_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "37d03562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_plot(size_title, size_xlabel, size_ylabel, size_legend, size_text, edge_size):\n",
    "\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(24, 12))\n",
    "\n",
    "    plot_color_A = \"orange\"\n",
    "    plot_color_B = \"royalblue\"\n",
    "\n",
    "    for i in range(norm_data_train.shape[0]):\n",
    "        if tr_label[i] == 0:\n",
    "            axes[0, 0].plot(norm_data_train[i, 0, :], color=plot_color_A)\n",
    "        else:\n",
    "            axes[0, 0].plot(norm_data_train[i, 0, :], color=plot_color_B)\n",
    "    axes[0, 0].set_title('Training Epoch 0', fontsize=size_title)\n",
    "\n",
    "\n",
    "    warped_training_data = centralized_functions_plot(5, device, processed_data_train)\n",
    "    for i in range(warped_training_data.shape[0]):\n",
    "        if tr_label[i] == 0:\n",
    "            axes[0, 1].plot(warped_training_data[i, :], color=plot_color_A)\n",
    "        else:\n",
    "            axes[0, 1].plot(warped_training_data[i, :], color=plot_color_B)\n",
    "    axes[0, 1].set_title('Training Epoch 5', fontsize=size_title)\n",
    "\n",
    "\n",
    "    warped_training_data = centralized_functions_plot(15, device, processed_data_train)\n",
    "    for i in range(warped_training_data.shape[0]):\n",
    "        if tr_label[i] == 0:\n",
    "            axes[0, 2].plot(warped_training_data[i, :], color=plot_color_A)\n",
    "        else:\n",
    "            axes[0, 2].plot(warped_training_data[i, :], color=plot_color_B)\n",
    "    axes[0, 2].set_title('Training Epoch 15', fontsize=size_title)\n",
    "\n",
    "\n",
    "    warped_training_data = centralized_functions_plot(30, device, processed_data_train)\n",
    "    for i in range(warped_training_data.shape[0]):\n",
    "        if tr_label[i] == 0:\n",
    "            axes[0, 3].plot(warped_training_data[i, :], color=plot_color_A)\n",
    "        else:\n",
    "            axes[0, 3].plot(warped_training_data[i, :], color=plot_color_B)\n",
    "    axes[0, 3].set_title('Training Epoch 30', fontsize=size_title)\n",
    "\n",
    "\n",
    "    axes[1, 0].plot(train_cor, label = r\"Cor\")\n",
    "    axes[1, 0].set_title('Training coeficient corelation', fontsize=size_title)\n",
    "    axes[1, 0].set_xlabel('Epoch', fontsize=size_xlabel)\n",
    "    axes[1, 0].set_ylabel('Corelation', fontsize=size_ylabel)\n",
    "    axes[1, 0].set_ylim([0,1])\n",
    "    axes[1, 0].legend(loc='lower right', fontsize=size_legend)\n",
    "\n",
    "\n",
    "    lns1 = axes[1, 1].plot(torch.tensor(train_Q_loss), label = r\"$Q_{Reg}$\")\n",
    "    axes2 = axes[1, 1].twinx()\n",
    "    lns2 = axes2.plot(torch.tensor(train_TV), label = \"TV\", color = \"orange\")\n",
    "    axes2.set_ylabel(\"TV\", rotation=270)\n",
    "    axes2.set_ylim([-20,520])\n",
    "    axes[1, 1].set_title(r\"$Q_{Reg}$ and TV\")\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel(r\"$Q_{Reg}$\")\n",
    "    axes[1, 1].set_ylim([-0.6,15.6])\n",
    "    lns = lns1 + lns2\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    axes[1, 1].legend(lns, labs, loc='best')\n",
    "\n",
    "\n",
    "    axes[1, 2].plot(loss_train,label='Training loss')\n",
    "    axes[1, 2].plot(left_loss_train,label='Registration loss')\n",
    "    axes[1, 2].plot(right_loss_train,label='Classification loss')\n",
    "    axes[1, 2].set_title('Training Loss', fontsize=size_title)\n",
    "    axes[1, 2].set_ylabel('Value', fontsize=size_ylabel)\n",
    "    axes[1, 2].set_xlabel('Epochs', fontsize=size_xlabel)\n",
    "    axes[1, 2].legend(fontsize=size_legend)\n",
    "\n",
    "\n",
    "    axes[1, 3].plot(torch.tensor(train_acc),label = \"Accuracy\")\n",
    "    axes[1, 3].plot(torch.tensor(train_F1),label = \"F1 score\")\n",
    "    axes[1, 3].set_title('Training Accuracy and F1 score', fontsize=size_title)\n",
    "    axes[1, 3].set_ylabel('Value', fontsize=size_ylabel)\n",
    "    axes[1, 3].set_xlabel('Epochs', fontsize=size_xlabel)\n",
    "    axes[1, 3].set_ylim([0,1.05])\n",
    "    axes[1, 3].legend(loc='lower right', fontsize=size_legend)\n",
    "\n",
    "\n",
    "    for i in range(norm_data_test.shape[0]):\n",
    "        if te_label[i] == 0:\n",
    "            axes[2, 0].plot(norm_data_test[i, 0, :], color=plot_color_A)\n",
    "        else:\n",
    "            axes[2, 0].plot(norm_data_test[i, 0, :], color=plot_color_B)\n",
    "    axes[2, 0].set_title('Raw Test data', fontsize=size_title)\n",
    "\n",
    "\n",
    "    warped_test_data = centralized_functions_plot(0, device, processed_data_test, best=True)\n",
    "    for i in range(warped_test_data.shape[0]):\n",
    "        if te_label[i] == 0:\n",
    "            axes[2, 1].plot(warped_test_data[i, :], color=plot_color_A)\n",
    "        else:\n",
    "            axes[2, 1].plot(warped_test_data[i, :], color=plot_color_B)\n",
    "    axes[2, 1].set_title('Aligned Test data', fontsize=size_title)\n",
    "\n",
    "\n",
    "    axes[2, 2].text(0.2, 0.5, r\"$Q_{reg}$ = \" + f'{test_Q_loss:.4f}' + \"\\nTV = \" + f'{test_TV:.4f}' + \n",
    "                    \"\\nAcc = \" + f'{test_acc*100:.2f}%' + \"\\nF1 = \" + f'{test_F1:.4f}%',\n",
    "              ha='center', va='center', fontsize = size_text, transform=axes[2, 2].transAxes)\n",
    "    axes[2, 2].set_xticks([])\n",
    "    axes[2, 2].set_yticks([])\n",
    "    \n",
    "    for spine in axes[2, 2].spines.values():\n",
    "        spine.set_visible(False)\n",
    "    \n",
    "\n",
    "    axes[2, 3].axis('off')\n",
    "\n",
    "    for ax in axes.flat:\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_linewidth(edge_size)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75215c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_plot(size_title, size_xlabel, size_ylabel, size_legend, size_text, edge_size)\n",
    "result_plot(18,15,15,15,24,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6bedc5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_plot_notitle(size_title, size_ylabel, size_legend, size_text, edge_size):\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(24, 12))\n",
    "\n",
    "    plot_color_A = \"orange\"\n",
    "    plot_color_B = \"royalblue\"\n",
    "\n",
    "    for i in range(norm_data_train.shape[0]):\n",
    "        if tr_label[i] == 0:\n",
    "            axes[0, 0].plot(norm_data_train[i, 0, :], color=plot_color_A)\n",
    "        else:\n",
    "            axes[0, 0].plot(norm_data_train[i, 0, :], color=plot_color_B)\n",
    "    axes[0, 0].set_xlabel('(a) Epoch 0', fontsize=size_title)\n",
    "\n",
    "    warped_training_data = centralized_functions_plot(5, device, processed_data_train)\n",
    "    for i in range(warped_training_data.shape[0]):\n",
    "        if tr_label[i] == 0:\n",
    "            axes[0, 1].plot(warped_training_data[i, :], color=plot_color_A)\n",
    "        else:\n",
    "            axes[0, 1].plot(warped_training_data[i, :], color=plot_color_B)\n",
    "    axes[0, 1].set_xlabel('(b) Epoch 5', fontsize=size_title)\n",
    "\n",
    "    warped_training_data = centralized_functions_plot(15, device, processed_data_train)\n",
    "    for i in range(warped_training_data.shape[0]):\n",
    "        if tr_label[i] == 0:\n",
    "            axes[0, 2].plot(warped_training_data[i, :], color=plot_color_A)\n",
    "        else:\n",
    "            axes[0, 2].plot(warped_training_data[i, :], color=plot_color_B)\n",
    "    axes[0, 2].set_xlabel('(c) Epoch 15', fontsize=size_title)\n",
    "\n",
    "    warped_training_data = centralized_functions_plot(30, device, processed_data_train)\n",
    "    for i in range(warped_training_data.shape[0]):\n",
    "        if tr_label[i] == 0:\n",
    "            axes[0, 3].plot(warped_training_data[i, :], color=plot_color_A)\n",
    "        else:\n",
    "            axes[0, 3].plot(warped_training_data[i, :], color=plot_color_B)\n",
    "    axes[0, 3].set_xlabel('(e) Epoch 30', fontsize=size_title)\n",
    "\n",
    "    axes[1, 0].plot(train_cor, label = r\"Cor\")\n",
    "    axes[1, 0].set_xlabel('(e) Coef cor vs. Epoch', fontsize=size_title)\n",
    "    axes[1, 0].set_ylabel('Corelation', fontsize=size_ylabel)\n",
    "    axes[1, 0].set_ylim([0,1])\n",
    "    axes[1, 0].legend(loc='lower right', fontsize=size_legend)\n",
    "\n",
    "    lns1 = axes[1, 1].plot(torch.tensor(train_Q_loss), label = r\"$\\Delta Q_{Reg}$\")\n",
    "    axes2 = axes[1, 1].twinx()\n",
    "    lns2 = axes2.plot(torch.tensor(train_TV), label = \"TV\", color = \"orange\")\n",
    "    axes2.set_ylabel(\"TV\", rotation=270, fontsize=size_ylabel)\n",
    "    axes2.set_ylim([-20,520])\n",
    "    axes[1, 1].set_xlabel(\"(f) \" + r\"$\\Delta Q_{Reg}$ & TV vs. Epoch\", fontsize=size_title)\n",
    "    axes[1, 1].set_ylabel(r\"$\\Delta Q_{Reg}$\", fontsize=size_ylabel)\n",
    "    axes[1, 1].set_ylim([-0.6,15.6])\n",
    "    lns = lns1 + lns2\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    axes[1, 1].legend(lns, labs, loc='best', fontsize=size_legend)\n",
    "\n",
    "    axes[1, 2].plot(loss_train,label='Training loss')\n",
    "    axes[1, 2].plot(left_loss_train,label='Registration loss')\n",
    "    axes[1, 2].plot(right_loss_train,label='Classification loss')\n",
    "    axes[1, 2].set_xlabel('(g) Loss vs. Epochs', fontsize=size_title)\n",
    "    axes[1, 2].legend(fontsize=size_legend)\n",
    "\n",
    "    axes[1, 3].plot(torch.tensor(train_acc),label = \"Accuracy\")\n",
    "    axes[1, 3].plot(torch.tensor(train_F1),label = \"F1 score\")\n",
    "    axes[1, 3].set_xlabel('(h) Acc & F1 vs. Epochs', fontsize=size_title)\n",
    "    axes[1, 3].set_ylim([0,1.05])\n",
    "    axes[1, 3].legend(loc='lower right', fontsize=size_legend)\n",
    "\n",
    "    for i in range(norm_data_test.shape[0]):\n",
    "        if te_label[i] == 0:\n",
    "            axes[2, 0].plot(norm_data_test[i, 0, :], color=plot_color_A)\n",
    "        else:\n",
    "            axes[2, 0].plot(norm_data_test[i, 0, :], color=plot_color_B)\n",
    "    axes[2, 0].set_xlabel('Test  Raw', fontsize=size_title)\n",
    "\n",
    "    warped_test_data = centralized_functions_plot(0, device, processed_data_test, best=True)\n",
    "    for i in range(warped_test_data.shape[0]):\n",
    "        if te_label[i] == 0:\n",
    "            axes[2, 1].plot(warped_test_data[i, :], color=plot_color_A)\n",
    "        else:\n",
    "            axes[2, 1].plot(warped_test_data[i, :], color=plot_color_B)\n",
    "    axes[2, 1].set_xlabel('Test  Aligned', fontsize=size_title)\n",
    "\n",
    "    axes[2, 2].text(0.1, 0.5, r\"$Q_{reg}$ = \" + f'{test_Q_loss:.4f}' + \"\\n  TV = \" + f'{test_TV:.3f}' + \n",
    "                    \"\\n Acc = \" + f'{test_acc*100:.2f}%' + \"\\n  F1 = \" + f'{test_F1:.4f}',\n",
    "              ha='center', va='center', fontsize = size_text, transform=axes[2, 2].transAxes)\n",
    "    axes[2, 2].set_xticks([])\n",
    "    axes[2, 2].set_yticks([])\n",
    "    \n",
    "    for spine in axes[2, 2].spines.values():\n",
    "        spine.set_visible(False)\n",
    "    \n",
    "    axes[2, 3].axis('off')\n",
    "\n",
    "    for ax in axes.flat:\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_linewidth(edge_size)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea29786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_plot_notitle(size_title, size_ylabel, size_legend, size_text, edge_size):\n",
    "result_plot_notitle(18,15,15,24,2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
